{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "Until recently, AnVIL data has been hosted and shared from Terra workspaces. Depending on the AnVIL dataset/study, the data tables in the workspace have varying schemas. In an effort to ingest all of the AnVIL datasets into TDR we had to find a way to create a common schema across all AnVIL datasets for the successful development of the AnVIL browser.\n",
    "\n",
    "When a user creates a cohort from the AnVIL browser, the data that is handed off to the workspace is a subset of data called the Findability Subset - that is to say that there are additional fields and information that remain in the TDR snapshot that the user will not see in their destination workspaces.\n",
    "\n",
    "### What does the notebook do?\n",
    "At point of hand-off, the user will see data tables with information that is representative of the Findability Subset. To retrieve the additional information, the non-Findability Subset (NFS), they must execute the notebook within the workspace to which they did their hand-off.\n",
    "\n",
    "The result of the notebook should be data tables that are organized the same as the original Terra AnVIL workspace as well as the tables that were created as views for the purposes of the AnVIL browser.\n",
    "\n",
    "### How to run the notebook?\n",
    "1. Start the notebook environment.\n",
    "2. Once running, click Cell → Run All.\n",
    "3. The user should see stdout that displays the status of the notebook’s actions as it performs its NFS data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 3 - 2/02/2023 - SC\n"
     ]
    }
   ],
   "source": [
    "print(f\"version 3 - 2/02/2023 - SC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports relevant packages. (Shift + Enter) to execute.\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from firecloud import api as fapi\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import csv\n",
    "import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "import requests\n",
    "from google.cloud import bigquery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Sets up workspace environment variables. (Shift + Enter) to execute.\n",
    "ws_project = os.environ['WORKSPACE_NAMESPACE']\n",
    "ws_name = os.environ['WORKSPACE_NAME']\n",
    "ws_bucket = os.environ['WORKSPACE_BUCKET']\n",
    "google_project = os.environ['GOOGLE_PROJECT']\n",
    "\n",
    "print(ws_project + \"\\n\" + ws_name + \"\\n\" + \"bucket: \" + ws_bucket + \"\\n\" + \"google project: \" + google_project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0,
     1,
     11,
     25,
     42,
     59,
     77,
     95,
     114,
     128,
     164,
     176,
     200,
     244,
     273,
     285,
     310
    ]
   },
   "outputs": [],
   "source": [
    "# functions. (Shift + Enter) to execute.\n",
    "def get_access_token():\n",
    "    \"\"\"Get access token.\"\"\"\n",
    "\n",
    "    scopes = [\"https://www.googleapis.com/auth/userinfo.profile\", \"https://www.googleapis.com/auth/userinfo.email\", \"openid\"]\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    credentials = credentials.create_scoped(scopes)\n",
    "\n",
    "    return credentials.get_access_token().access_token\n",
    "\n",
    "\n",
    "def get_query_results(query):\n",
    "    \"\"\"Performs a BQ query.\"\"\"\n",
    "    \n",
    "    # create BQ connection\n",
    "    bq = bigquery.Client(google_project)\n",
    "    \n",
    "    executed_query = bq.query(query)\n",
    "    result = executed_query.result()\n",
    "    \n",
    "    df_result = result.to_dataframe()\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "\n",
    "def get_dataset_info(dataset_id):\n",
    "    \"\"\"\"Get dataset details from retrieveDataset API given a datasetID.\"\"\"\n",
    "    \n",
    "    uri = f\"https://data.terra.bio/api/repository/v1/datasets/{dataset_id}?include=SCHEMA%2CPROFILE%2CDATA_PROJECT%2CSTORAGE\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(), \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved details for dataset with datasetID {dataset_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_snapshot_info(snapshot_id):\n",
    "    \"\"\"\"Get dataset details from retrieveDataset API given a datasetID.\"\"\"\n",
    "    \n",
    "    uri = f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}?include=\"\n",
    "\n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(), \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved details for dataset with datasetID {snapshot_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_dataset_access_info(dataset_id):\n",
    "    \"\"\"\"Get dataset access details from retrieveDataset API given a datasetID.\"\"\"\n",
    "    \n",
    "    uri = f\"https://data.terra.bio/api/repository/v1/datasets/{dataset_id}?include=ACCESS_INFORMATION\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(),\n",
    "               \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved access information for dataset with datasetID {dataset_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_snapshot_access_info(snapshot_id):\n",
    "    \"\"\"Get snapshot access information from retrieveSnapshot API given a snapshotID\"\"\"\n",
    "    \n",
    "    uri = f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}?include=ACCESS_INFORMATION\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(),\n",
    "               \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved access information for snapshot with snapshotID {snapshot_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_fq_table(entity_id, table_name, entity_type='dataset'):\n",
    "    \"\"\"Given a datset or snapshot id, table name, and entity type {dataset,snapshot}, retrieve its fully qualified BQ table name\"\"\"\n",
    "    if entity_type == 'dataset':\n",
    "        access_info = get_dataset_access_info(entity_id)\n",
    "    elif entity_type == 'snapshot':\n",
    "        access_info = get_snapshot_access_info(entity_id)\n",
    "\n",
    "    project_id = access_info['accessInformation']['bigQuery']['projectId']\n",
    "    tables = access_info['accessInformation']['bigQuery']['tables']\n",
    "\n",
    "    # pull out desired table\n",
    "    table_fq = None  # fq = fully qualified name, i.e. project.dataset.table\n",
    "    for table_info in tables:\n",
    "        if table_info['name'] == table_name:\n",
    "            table_fq = table_info['qualifiedName'] \n",
    "    \n",
    "    return table_fq\n",
    "    \n",
    "    \n",
    "def create_data_table(entities_tsv):\n",
    "    \"\"\"Create a Terra data table given a Terra load tsv.\"\"\"\n",
    "    \n",
    "    response = fapi.upload_entities_tsv(ws_project, ws_name, entities_tsv, model='flexible')\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code not in [200, 202]:\n",
    "        print(f\"WARNING: Loading {entities_tsv} to Terra workspace failed: \\n {response.text}\")\n",
    "        return status_code\n",
    "    \n",
    "    print(f\"Finished uploading {entities_tsv} to Terra workspace as a data table.\")\n",
    "    return status_code\n",
    "    \n",
    "    \n",
    "def group_list(input_df, group_column):\n",
    "    \"\"\"Group data by their source data table names.\"\"\"\n",
    "    \n",
    "    # create df grouping by input column value\n",
    "    grouped_df = pd.DataFrame(input_df).groupby(group_column).agg(list)\n",
    "        \n",
    "    for index, row in grouped_df.iterrows():\n",
    "        group_dict = row.to_dict()\n",
    "        group_df = pd.DataFrame(group_dict)\n",
    "        \n",
    "        # remove duplicate rows in single data table's dataframe        \n",
    "        # sort by datarepo_row_id\n",
    "        group_df.sort_values(\"datarepo_row_id\", inplace=True)\n",
    "        # drop all but one of the duplicate rows\n",
    "        group_df.drop_duplicates(subset=\"datarepo_row_id\", keep=\"first\", inplace=True)\n",
    "    \n",
    "        if index == \"file_inventory\":\n",
    "            original_col = \"file_id\"\n",
    "            etype_name = \"entity:file_inventory_id\"\n",
    "        elif index == \"workspace_attributes\":\n",
    "            original_col = \"datarepo_row_id\"\n",
    "            etype_name = \"entity:workspace_attributes_id\"\n",
    "        else:\n",
    "            original_col = f\"{index}_id\"\n",
    "            etype_name = f\"entity:{index}_id\"\n",
    "            \n",
    "        # rename the id column with entity:[]_id and move to first column position\n",
    "        group_df = group_df.rename(columns={original_col: etype_name})\n",
    "        first_column = group_df.pop(etype_name)\n",
    "        group_df.insert(0, etype_name, first_column)\n",
    "\n",
    "        # write df to tsv and load to Terra\n",
    "        filename = write_df_to_tsv(group_df, index)\n",
    "        create_data_table(filename)\n",
    "\n",
    "        \n",
    "def write_df_to_tsv(input_df, entity_name):\n",
    "    \"\"\"Create a Terra load tsv given a list of dictionaries.\"\"\"\n",
    "    \n",
    "    outfile_name = f\"{entity_name}.tsv\"\n",
    "\n",
    "    # rename the first column to have a new entity_name (entity:entity_name_combined:id)\n",
    "    input_df.to_csv(outfile_name, sep='\\t', index=False)\n",
    "    \n",
    "    print(f\"Finished writing {entity_name} as {outfile_name} to Terra load file.\")\n",
    "    return outfile_name\n",
    "\n",
    "\n",
    "def rename_etype(original_etype):\n",
    "    \"\"\"Rename PFB handoff tables with `anvil_` prefix.\"\"\"\n",
    "    \n",
    "    uri = f\"https://rawls.dsde-prod.broadinstitute.org/api/workspaces/{ws_project}/{ws_name}/entityTypes/{original_etype}\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(), \"accept\": \"*/*\", \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # capture response from API and parse out status code\n",
    "    new_etype = f\"anvil_{original_etype}\"\n",
    "    d = {\"newName\": new_etype}\n",
    "    data = json.dumps(d)\n",
    "    response = requests.patch(uri, headers=headers, data=data)\n",
    "    \n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 204:\n",
    "        print(f\"Warning: Renaming table from {original_etype} to {new_etype} failed. See error for details:\\n\")\n",
    "        print(response.text)\n",
    "        return status_code, response.text\n",
    "    \n",
    "    print(f\"Successfully renamed table {original_etype} to {new_etype}. \\n\\n\")\n",
    "    return status_code, response.text\n",
    "\n",
    "\n",
    "def query_source_tables(query_terms_dict):\n",
    "    \"\"\"Query source TDR dataset tables by datarepo_row_id to get NFS data.\"\"\"\n",
    "\n",
    "    nfs = pd.DataFrame() # empty df that will contain all entities\n",
    "    for key, value in query_terms_dict.items():\n",
    "        src_table_name = key.split(\":\")[0]\n",
    "        src_snapshot_id = key.split(\":\")[1]\n",
    "        src_datarepo_row_ids = list(set(value)) # set to remove duplicate values\n",
    "        \n",
    "        # chunk src_datarepo_row_ids to handle bq query string being > 1024 characters\n",
    "        chunk_size = 10000 # handles if list len > chunk_size or odd number of values\n",
    "        chunked_src_datarepo_row_ids = [src_datarepo_row_ids[i:i + chunk_size] for i in range(0, len(src_datarepo_row_ids), chunk_size)]\n",
    "        \n",
    "        # get bq qualified table name for bq query\n",
    "        fq_src_table_name = get_fq_table(src_snapshot_id, src_table_name, entity_type='snapshot')\n",
    "        \n",
    "        results = pd.DataFrame() # df to capture results of chunked queries\n",
    "        # for each chunk (20 datarepo_row_ids)\n",
    "        for chunk in chunked_src_datarepo_row_ids:\n",
    "            src_datarepo_row_ids = \"('\" + \"','\".join(list(chunk)) + \"')\"\n",
    "            query = f\"\"\"SELECT * FROM `{fq_src_table_name}` WHERE datarepo_row_id IN {src_datarepo_row_ids}\"\"\"\n",
    "            chunk_results = get_query_results(query) # get df of results\n",
    "            \n",
    "            # concatenate results from chunk with previous chunk results\n",
    "            results = pd.concat([results, chunk_results], axis=0)\n",
    "\n",
    "        # add the source table name into dictionary to organize into original workspace table structure\n",
    "        results[\"source_table_name\"] = src_table_name\n",
    "\n",
    "        # if any of the df column names have Terra reserved names, rename to load to terra without failure\n",
    "        # ex: file_inventory table in snapshot has `name` column and fails to load to Terra\n",
    "        col_new_name = \"_name_\"\n",
    "        if 'name' in results.columns:\n",
    "            if col_new_name not in results.columns:\n",
    "                results = results.rename(columns={\"name\": col_new_name})\n",
    "            else:\n",
    "                raise ValueError(f\"{col_new_name} already exists in {src_table_name}. Pick another name and retry.\")\n",
    "\n",
    "        # combined results to nfs and nfs dictionary to a list\n",
    "        nfs = pd.concat([nfs, results], axis=0)\n",
    "\n",
    "    return nfs\n",
    "    \n",
    "    \n",
    "def get_nfs_data(entity_data):\n",
    "    \"\"\"Return non findability subset data mapping to findability subset inputs.\"\"\"\n",
    "    \n",
    "    sources = {} # capture source details for each datarepo_row_id to be queried\n",
    "    for index, row in entity_data.iterrows():\n",
    "        # dictionary of findability subset values for a single row\n",
    "        fs_dict = row.to_dict()\n",
    "        src_snapshot_id = row[\"pfb:source_datarepo_snapshot_id\"]\n",
    "        # list of all source data repo row ids\n",
    "        src_drr_ids = row[\"pfb:source_datarepo_row_ids\"] \n",
    "        # for each source_datarepo_row --> [sequencing:f9e70781-gjt6-422d-a93a-733ac060cb05]\n",
    "        for src_drr_id in src_drr_ids:\n",
    "            drr_id = src_drr_id.split(\":\")[1] # f9e70781-gjt6-422d-a93a-733ac060cb05\n",
    "            src_table_name = src_drr_id.split(\":\")[0] # sequencing\n",
    "            snap_table = f\"{src_table_name}:{src_snapshot_id}\" # unique pair\n",
    "            \n",
    "            # add datarepo_row_ids based on unique key\n",
    "            if snap_table not in sources:\n",
    "                sources[snap_table] = set()\n",
    "            \n",
    "            \n",
    "            sources[snap_table].add(drr_id)\n",
    "    \n",
    "    # query and get list of results for single entity table\n",
    "    entity_nfs_results = query_source_tables(sources)\n",
    "    \n",
    "    return entity_nfs_results\n",
    "        \n",
    "\n",
    "def get_entity_df(ws_project, ws_name, etype):\n",
    "    \"\"\"Get tsv file for a given entity in a Terra workspace.\"\"\"\n",
    "        \n",
    "    response = fapi.get_entities_tsv(ws_project, ws_name, etype, model='flexible')\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Error getting {etype} table data from workspace: {response.text}\")\n",
    "\n",
    "    df = pd.read_csv(StringIO(response.text), sep='\\t')\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "def get_entities(ws_project, ws_name):\n",
    "    \"\"\"Get applicable entity types for nfs extraction.\"\"\"\n",
    "    \n",
    "    # API call to get all entity types in workspace\n",
    "    res_etypes = fapi.list_entity_types(ws_project, ws_name)\n",
    "    dict_all_etypes = json.loads(res_etypes.text)\n",
    "\n",
    "    # get non-set entities and add to list\n",
    "    single_etypes_list = []\n",
    "    single_etypes_list = [key for key in dict_all_etypes.keys() if not key.endswith(\"_set\")]\n",
    "    \n",
    "    # filter single etypes to ones applicable for NFS extraction.\n",
    "    # do not attempt to analyze and rename data in these tables or if they were already renamed with \"anvil_\"\n",
    "    # tables in the the ignore list are resulting NFS tables that do not have PFB values\n",
    "    ignore = [\"sample\", \"file_inventory\", \"subject\", \"workspace_attributes\", \"sequencing\", \"family\",\n",
    "                 \"participant\"]\n",
    "    \n",
    "    fltrd_entities = [table for table in single_etypes_list if table not in ignore and not table.startswith(\"anvil_\")] \n",
    "\n",
    "    print(f\"List of entity types that will be updated, if applicable:\")\n",
    "    print('\\n'.join(['\\t' * 7 + c for c in fltrd_entities]))\n",
    "    \n",
    "    return fltrd_entities\n",
    "\n",
    "\n",
    "def get_ws_nfs_data(ws_project, ws_name):\n",
    "    \"\"\"For each entity in the workspace, create combined findability and non-findability subset tables.\"\"\"\n",
    "\n",
    "    # get list of viable tables to run NFS extraction\n",
    "    entities = get_entities(ws_project, ws_name)\n",
    "#     entities = [\"activities\", \"biosamples\"]\n",
    "    \n",
    "    all_entities_nfs = pd.DataFrame()\n",
    "    for etype in entities:\n",
    "        print(f\"Starting: {etype}\")\n",
    "        \n",
    "        # call Terra API to get table data\n",
    "        print(f\"Extracting {etype}'s findability subset data from Terra data tables.\")\n",
    "        response = get_entity_df(ws_project, ws_name, etype)\n",
    "        # set column of source datarepo_row_ids to type array (come in as string)\n",
    "        response['pfb:source_datarepo_row_ids'] = response['pfb:source_datarepo_row_ids'].map(ast.literal_eval)\n",
    "        \n",
    "        # get df of all nfs data for entity\n",
    "        print(f\"Starting extraction of {etype}'s non-findability subset data.\")\n",
    "        entity_nfs = get_nfs_data(response)\n",
    "        \n",
    "        # concatenate single entity df to all entities df\n",
    "        all_entities_nfs = pd.concat([all_entities_nfs, entity_nfs], axis=0)\n",
    "        print(f\"Finished extraction of {etype}'s non-findability subset data.\")\n",
    "    \n",
    "        # rename original etype with `anvil_` prefix to match TDR\n",
    "        # TODO: consider how to handle tables that have already been renamed or modified from a previous notebook run\n",
    "        print(f\"Renaming {etype} to anvil_{etype}.\")\n",
    "        rename_etype(etype)\n",
    "    \n",
    "    print(f\"Starting creation of Terra data tables.\")\n",
    "    # organize all entities' nfs data by source_table_name values and then create tsv files and ingest into Terra\n",
    "    df = group_list(all_entities_nfs, \"source_table_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# RUN FUNCTION\n",
    "get_ws_nfs_data(ws_project, ws_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
