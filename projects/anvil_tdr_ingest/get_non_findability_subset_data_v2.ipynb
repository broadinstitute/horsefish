{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: RUN THIS CELL and then click Kernel -> Restart.\n",
    "# Once restarted, you DO NOT have to run this cell again.\n",
    "# TODO: need to get the notebook env's Firecloud to be updated to the newest version\n",
    "!pip install --upgrade firecloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports relevant packages. (Shift + Enter) to execute.\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from firecloud import api as fapi\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import csv\n",
    "import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "import requests\n",
    "from google.cloud import bigquery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Sets up workspace environment variables. (Shift + Enter) to execute.\n",
    "ws_project = os.environ['WORKSPACE_NAMESPACE']\n",
    "ws_name = os.environ['WORKSPACE_NAME']\n",
    "ws_bucket = os.environ['WORKSPACE_BUCKET']\n",
    "google_project = os.environ['GOOGLE_PROJECT']\n",
    "\n",
    "print(ws_project + \"\\n\" + ws_name + \"\\n\" + \"bucket: \" + ws_bucket + \"\\n\" + \"google project: \" + google_project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     1,
     11,
     25,
     42,
     59,
     77,
     95,
     128,
     159,
     171,
     181,
     199,
     230,
     257,
     269,
     292
    ]
   },
   "outputs": [],
   "source": [
    "# functions. (Shift + Enter) to execute.\n",
    "def get_access_token():\n",
    "    \"\"\"Get access token.\"\"\"\n",
    "\n",
    "    scopes = [\"https://www.googleapis.com/auth/userinfo.profile\", \"https://www.googleapis.com/auth/userinfo.email\", \"openid\"]\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    credentials = credentials.create_scoped(scopes)\n",
    "\n",
    "    return credentials.get_access_token().access_token\n",
    "\n",
    "\n",
    "def get_query_results(query):\n",
    "    \"\"\"Performs a BQ query.\"\"\"\n",
    "    \n",
    "    # create BQ connection\n",
    "    bq = bigquery.Client(google_project)\n",
    "    \n",
    "    executed_query = bq.query(query)\n",
    "    result = executed_query.result()\n",
    "    \n",
    "    df_result = result.to_dataframe()\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "\n",
    "def get_dataset_info(dataset_id):\n",
    "    \"\"\"\"Get dataset details from retrieveDataset API given a datasetID.\"\"\"\n",
    "    \n",
    "    uri = f\"https://data.terra.bio/api/repository/v1/datasets/{dataset_id}?include=SCHEMA%2CPROFILE%2CDATA_PROJECT%2CSTORAGE\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(), \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved details for dataset with datasetID {dataset_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_snapshot_info(snapshot_id):\n",
    "    \"\"\"\"Get dataset details from retrieveDataset API given a datasetID.\"\"\"\n",
    "    \n",
    "    uri = f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}?include=\"\n",
    "\n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(), \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved details for dataset with datasetID {snapshot_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_dataset_access_info(dataset_id):\n",
    "    \"\"\"\"Get dataset access details from retrieveDataset API given a datasetID.\"\"\"\n",
    "    \n",
    "    uri = f\"https://data.terra.bio/api/repository/v1/datasets/{dataset_id}?include=ACCESS_INFORMATION\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(),\n",
    "               \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved access information for dataset with datasetID {dataset_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_snapshot_access_info(snapshot_id):\n",
    "    \"\"\"Get snapshot access information from retrieveSnapshot API given a snapshotID\"\"\"\n",
    "    \n",
    "    uri = f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}?include=ACCESS_INFORMATION\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(),\n",
    "               \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved access information for snapshot with snapshotID {snapshot_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_fq_table(entity_id, table_name, entity_type='dataset'):\n",
    "    \"\"\"Given a datset or snapshot id, table name, and entity type {dataset,snapshot}, retrieve its fully qualified BQ table name\"\"\"\n",
    "    if entity_type == 'dataset':\n",
    "        access_info = get_dataset_access_info(entity_id)\n",
    "    elif entity_type == 'snapshot':\n",
    "        access_info = get_snapshot_access_info(entity_id)\n",
    "\n",
    "    project_id = access_info['accessInformation']['bigQuery']['projectId']\n",
    "    tables = access_info['accessInformation']['bigQuery']['tables']\n",
    "\n",
    "    # pull out desired table\n",
    "    table_fq = None  # fq = fully qualified name, i.e. project.dataset.table\n",
    "    for table_info in tables:\n",
    "        if table_info['name'] == table_name:\n",
    "            table_fq = table_info['qualifiedName'] \n",
    "    \n",
    "    return table_fq\n",
    "    \n",
    "    \n",
    "def create_data_table(entities_tsv):\n",
    "    \"\"\"Create a Terra data table given a Terra load tsv.\"\"\"\n",
    "    \n",
    "    response = fapi.upload_entities_tsv(ws_project, ws_name, entities_tsv, model='flexible')\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code not in [200, 202]:\n",
    "        print(f\"WARNING: Loading {entities_tsv} to Terra workspace failed: \\n {response.text}\")\n",
    "        return status_code\n",
    "    \n",
    "    print(f\"Finished uploading {entities_tsv} to Terra workspace as a data table.\")\n",
    "    return status_code\n",
    "    \n",
    "    \n",
    "def group_list(input_list, group_column):\n",
    "    \"\"\"Group data by their source data table names.\"\"\"\n",
    "    \n",
    "    # create df grouping by input column value\n",
    "    grouped_df = pd.DataFrame(input_list).groupby(group_column).agg(list)\n",
    "    \n",
    "    for index, row in grouped_df.iterrows():\n",
    "        print(index)\n",
    "        group_dict = row.to_dict()\n",
    "        group_df = pd.DataFrame(group_dict)\n",
    "        \n",
    "        if index == \"file_inventory\":\n",
    "            original_col = \"file_id\"\n",
    "            etype_name = \"entity:file_inventory_id\"\n",
    "        elif index == \"workspace_attributes\":\n",
    "            original_col = \"datarepo_row_id\"\n",
    "            etype_name = \"entity:workspace_attributes_id\"\n",
    "        else:\n",
    "            original_col = f\"{index}_id\"\n",
    "            etype_name = f\"entity:{index}_id\"\n",
    "            \n",
    "        # rename the id column with entity:[]_id and move to first column\n",
    "        group_df = group_df.rename(columns={original_col: etype_name})\n",
    "        \n",
    "        first_column = group_df.pop(etype_name)\n",
    "        group_df.insert(0, etype_name, first_column)\n",
    "        \n",
    "        filename = write_df_to_tsv(group_df, index)\n",
    "        create_data_table(filename)\n",
    "\n",
    "        \n",
    "def write_df_to_tsv(input_df, entity_name):\n",
    "    \"\"\"Create a Terra load tsv given a list of dictionaries.\"\"\"\n",
    "    \n",
    "    outfile_name = f\"{entity_name}.tsv\"\n",
    "\n",
    "    # rename the first column to have a new entity_name (entity:entity_name_combined:id)\n",
    "    input_df.to_csv(outfile_name, sep='\\t', index=False)\n",
    "    \n",
    "    print(f\"Finished writing {entity_name} as {outfile_name} to Terra load file.\")\n",
    "    return outfile_name\n",
    "\n",
    "\n",
    "def delete_etype(ws_project, ws_name, etype):\n",
    "    \"\"\"Delete an entity table.\"\"\"\n",
    "    \n",
    "    response = fapi. delete_entities_of_type(ws_project, ws_name, etype)\n",
    "    # TODO: figure out how to handle renaming tables. \n",
    "    # Rename API does not have merge abilities.\n",
    "    # Currently, creating a new merged TSV and then deleting the original\n",
    "    # This might cause people to lose any additional columns that they have added.\n",
    "    \n",
    "\n",
    "def rename_etype(entity_df, etype):\n",
    "    \"\"\"Rename PFB handoff tables with `anvil_` prefix.\"\"\"\n",
    "    \n",
    "    renamed_etype = f\"anvil_{etype}\"\n",
    "    new_col_name = f\"entity:{renamed_etype}_id\"\n",
    "    \n",
    "    entity_df = entity_df.rename(columns={f\"entity:{etype}_id\": new_col_name})\n",
    "    \n",
    "    load_tsv = write_df_to_tsv(entity_df, renamed_etype)    \n",
    "    status_code = create_data_table(load_tsv)\n",
    "    \n",
    "    # if adding succeeded\n",
    "    if status_code in [200, 202]:\n",
    "        delete_etype(ws_project, ws_name, etype)\n",
    "    \n",
    "    print(f\"Finished renaming {etype} table to {renamed_etype}.\")\n",
    "\n",
    "\n",
    "def query_source_tables(query_terms_dict):\n",
    "    \"\"\"Query source TDR dataset tables by datarepo_row_id to get NFS data.\"\"\"\n",
    "\n",
    "    nfs = pd.DataFrame() # empty df that will contain all entities\n",
    "    for key, value in query_terms_dict.items():\n",
    "        src_table_name = key.split(\":\")[0]\n",
    "        src_snapshot_id = key.split(\":\")[1]\n",
    "        src_datarepo_row_ids = \"('\" + \"','\".join(list(value)) + \"')\"\n",
    "    \n",
    "        fq_src_table_name = get_fq_table(src_snapshot_id, src_table_name, entity_type='snapshot')    \n",
    "        query = f\"\"\"SELECT * FROM `{fq_src_table_name}` WHERE datarepo_row_id IN {src_datarepo_row_ids}\"\"\"\n",
    "        results = get_query_results(query)\n",
    "\n",
    "        # add the source table name into dictionary to organize into original workspace table structure\n",
    "        results[\"source_table_name\"] = src_table_name\n",
    "\n",
    "        # if any of the df column names have Terra reserved names, rename to load to terra without failure\n",
    "        # ex: file_inventory table in snapshot has `name` column and fails to load to Terra\n",
    "        col_new_name = \"_name_\"\n",
    "        if 'name' in results.columns:\n",
    "            if col_new_name not in results.columns:\n",
    "                results = results.rename(columns={\"name\": col_new_name})\n",
    "            else:\n",
    "                raise ValueError(f\"{col_new_name} already exists in {src_table_name}. Pick another name and retry.\")\n",
    "\n",
    "        # combined results to nfs and nfs dictionary to a list\n",
    "        nfs = pd.concat([nfs, results], axis=0)\n",
    "\n",
    "    return nfs\n",
    "    \n",
    "    \n",
    "def get_nfs_data(entity_data):\n",
    "    \"\"\"Return non findability subset data mapping to findability subset inputs.\"\"\"\n",
    "    \n",
    "    sources = {} # capture source details for each datarepo_row_id to be queried\n",
    "    for index, row in entity_data.iterrows():\n",
    "        # dictionary of findability subset values for a single row\n",
    "        fs_dict = row.to_dict()\n",
    "        src_snapshot_id = row[\"pfb:source_datarepo_snapshot_id\"]\n",
    "        # list of all source data repo row ids\n",
    "        src_drr_ids = row[\"pfb:source_datarepo_row_ids\"] \n",
    "        # for each source_datarepo_row --> [sequencing:f9e70781-gjt6-422d-a93a-733ac060cb05]\n",
    "        for src_drr_id in src_drr_ids:\n",
    "            drr_id = src_drr_id.split(\":\")[1] # f9e70781-gjt6-422d-a93a-733ac060cb05\n",
    "            src_table_name = src_drr_id.split(\":\")[0] # sequencing\n",
    "            snap_table = f\"{src_table_name}:{src_snapshot_id}\" # unique pair\n",
    "            \n",
    "            # add datarepo_row_ids based on unique key\n",
    "            if snap_table not in sources:\n",
    "                sources[snap_table] = set()\n",
    "            sources[snap_table].add(drr_id)\n",
    "    \n",
    "    # query and get list of results for single entity table\n",
    "    entity_nfs_results = query_source_tables(sources)\n",
    "    \n",
    "    return entity_nfs_results\n",
    "        \n",
    "\n",
    "def get_entity_df(ws_project, ws_name, etype):\n",
    "    \"\"\"Get tsv file for a given entity in a Terra workspace.\"\"\"\n",
    "        \n",
    "    response = fapi.get_entities_tsv(ws_project, ws_name, etype, model='flexible')\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Error getting {etype} table data from workspace: {response.text}\")\n",
    "\n",
    "    df = pd.read_csv(StringIO(response.text), sep='\\t')\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "def get_entities(ws_project, ws_name):\n",
    "    \"\"\"Get applicable entity types for nfs extraction.\"\"\"\n",
    "    \n",
    "    # API call to get all entity types in workspace\n",
    "    res_etypes = fapi.list_entity_types(ws_project, ws_name)\n",
    "    dict_all_etypes = json.loads(res_etypes.text)\n",
    "\n",
    "    # get non-set entities and add to list\n",
    "    single_etypes_list = []\n",
    "    single_etypes_list = [key for key in dict_all_etypes.keys() if not key.endswith(\"_set\")]\n",
    "    \n",
    "    # filter single etypes to ones applicable for NFS extraction.\n",
    "    # do not attempt to analyze and rename data in these tables or if they were already renamed with \"anvil_\"\n",
    "    blacklist = [\"sample\", \"file_inventory\", \"subject\", \"workspace_attributes\", \"sequencing\"]\n",
    "    \n",
    "    fltrd_entities = [table for table in single_etypes_list if table not in blacklist and not table.startswith(\"anvil_\")] \n",
    "\n",
    "    print(f\"List of entity types that will be updated, if applicable:\")\n",
    "    print('\\n'.join(['\\t' * 7 + c for c in fltrd_entities]))\n",
    "    \n",
    "    return fltrd_entities\n",
    "\n",
    "\n",
    "def get_ws_nfs_data(ws_project, ws_name):\n",
    "    \"\"\"For each entity in the workspace, create combined findability and non-findability subset tables.\"\"\"\n",
    "\n",
    "    # get list of viable tables to run NFS extraction\n",
    "    entities = get_entities(ws_project, ws_name)\n",
    "    \n",
    "    all_entities_nfs = pd.DataFrame()\n",
    "    for etype in entities:\n",
    "        print(f\"Starting: {etype}\")\n",
    "        \n",
    "        # call Terra API to get table data\n",
    "        print(f\"Extracting {etype}'s findability subset data from Terra data tables.\")\n",
    "        response = get_entity_df(ws_project, ws_name, etype)\n",
    "        # set column of source datarepo_row_ids to type array (come in as string)\n",
    "        response['pfb:source_datarepo_row_ids'] = response['pfb:source_datarepo_row_ids'].map(ast.literal_eval)\n",
    "        \n",
    "        # get df of all nfs data for entity\n",
    "        print(f\"Starting extraction of {etype}'s non-findability subset data.\")\n",
    "        entity_nfs = get_nfs_data(response)\n",
    "        \n",
    "        # concatenate single entity df to all entities df\n",
    "        all_entities_nfs = pd.concat([all_entities_nfs, entity_nfs], axis=0)\n",
    "        print(f\"Finished extraction of {etype}'s non-findability subset data. \\n\\n\")\n",
    "    \n",
    "        # rename original etype with `anvil_` prefix to match TDR\n",
    "        # TODO: consider how to handle tables that have already been renamed or modified from a previous notebook run\n",
    "        print(f\"Renaming {etype} to anvil_{etype}.\")\n",
    "        rename_etype(response, etype)\n",
    "    \n",
    "    print(f\"Starting creation of Terra data tables.\")\n",
    "    # organize all entities' nfs data by source_table_name values and then create tsv files and ingest into Terra\n",
    "    df = group_list(all_entities_nfs, \"source_table_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FUNCTION\n",
    "get_ws_nfs_data(ws_project, ws_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
