{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports and environment variables\n",
    "from datetime import datetime, tzinfo\n",
    "import json\n",
    "import os\n",
    "import pytz\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "from firecloud import api as fapi\n",
    "from firecloud import fccore  # FISS configurations\n",
    "from google.cloud import bigquery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from pprint import pprint\n",
    "\n",
    "# Get current workspace variables.\n",
    "ws_project = os.environ['WORKSPACE_NAMESPACE']\n",
    "ws_name = os.environ['WORKSPACE_NAME']\n",
    "ws_bucket = os.environ['WORKSPACE_BUCKET']\n",
    "gcp_project = os.environ['GOOGLE_PROJECT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     11
    ]
   },
   "outputs": [],
   "source": [
    "## user inputs required -- FILL IN THE ENVIRONMENT AND SUBMISSION ID FOR ARRAYS.WDL\n",
    "env = \"prod\" # [\"alpha\", \"prod\"]\n",
    "arrays_submission_id = 'b659c03b-78c6-40f4-a385-6f75f31308cd'\n",
    "\n",
    "verbose = False\n",
    "# these variables should not change often\n",
    "dataset_id = '6f2bb559-34ae-4ba0-b2ce-1d8be76ada9f'\n",
    "workflow_name = 'Arrays'\n",
    "\n",
    "# the workflow outputs as returned by Terra APIs don't match what we want to call them, so we define conversions here.\n",
    "# format is {\"WorkflowName.outputname\": \"tdr_field_name\"}\n",
    "workflow_outputs_dict = {\n",
    "    \"Arrays.chip_well_barcode_output\": \"chip_well_barcode_output\",\n",
    "    \"Arrays.analysis_version_number_output\": \"analysis_version_number_output\",\n",
    "    \"Arrays.gtc_file\": \"gtc_file\",\n",
    "    \"Arrays.arrays_variant_calling_control_metrics_file\": \"arrays_variant_calling_control_metrics_file\",\n",
    "    \"Arrays.arrays_variant_calling_detail_metrics_file\": \"arrays_variant_calling_detail_metrics_file\",\n",
    "    \"Arrays.arrays_variant_calling_summary_metrics_file\": \"arrays_variant_calling_summary_metrics_file\",\n",
    "    \"Arrays.baf_regress_metrics_file\": \"baf_regress_metrics_file\",\n",
    "    \"Arrays.fingerprint_detail_metrics_file\": \"fingerprint_detail_metrics_file\",\n",
    "    \"Arrays.fingerprint_summary_metrics_file\": \"fingerprint_summary_metrics_file\",\n",
    "    \"Arrays.genotype_concordance_contingency_metrics_file\": \"genotype_concordance_contingency_metrics_file\",\n",
    "    \"Arrays.genotype_concordance_detail_metrics_file\": \"genotype_concordance_detail_metrics_file\",\n",
    "    \"Arrays.genotype_concordance_summary_metrics_file\": \"genotype_concordance_summary_metrics_file\",\n",
    "    \"Arrays.last_modified_date\": \"last_modified_date\",\n",
    "    \"Arrays.output_vcf\": \"output_vcf\",\n",
    "    \"Arrays.output_vcf_index\": \"output_vcf_index\"\n",
    "}\n",
    "target_table_name = \"ArraysOutputsTable\"\n",
    "\n",
    "# determine environment and associated TDR base uri for API calls\n",
    "tdr_env_uri_dict = {\"alpha\": \"https://data.alpha.envs-terra.bio\",\n",
    "                    \"prod\": \"https://data.terra.bio\"}\n",
    "tdr_env_uri = tdr_env_uri_dict[env]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0,
     3,
     13,
     32,
     50,
     61,
     68,
     84,
     104,
     155,
     179,
     198,
     216
    ]
   },
   "outputs": [],
   "source": [
    "## functions\n",
    "\n",
    "# the following functions are updated to use alpha TDR swapper urs\n",
    "def get_access_token():\n",
    "    \"\"\"Get access token.\"\"\"\n",
    "\n",
    "    scopes = [\"https://www.googleapis.com/auth/userinfo.profile\", \"https://www.googleapis.com/auth/userinfo.email\"]\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    credentials = credentials.create_scoped(scopes)\n",
    "\n",
    "    return credentials.get_access_token().access_token\n",
    "\n",
    "\n",
    "def check_user():\n",
    "    \n",
    "    uri = f\"{tdr_env_uri}/api/repository/v1/register/user\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(),\n",
    "               \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        print(f\"Check user request failed\")\n",
    "        pprint(response.text)\n",
    "        return\n",
    "\n",
    "    print(f\"Successfully retrieved user info.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_dataset_info(dataset_id):\n",
    "    \"\"\"\"Get dataset details from retrieveDataset API given a datasetID.\"\"\"\n",
    "\n",
    "    uri = f\"{tdr_env_uri}/api/repository/v1/datasets/{dataset_id}?include=SCHEMA%2CPROFILE%2CDATA_PROJECT%2CSTORAGE\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(),\n",
    "               \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved details for dataset with datasetID {dataset_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_dataset_access_info(dataset_id):\n",
    "    \"\"\"\"Get dataset access details from retrieveDataset API given a datasetID.\"\"\"\n",
    "    \n",
    "    uri = f\"{tdr_env_uri}/api/repository/v1/datasets/{dataset_id}?include=ACCESS_INFORMATION\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(),\n",
    "               \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved access information for dataset with datasetID {dataset_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_snapshot_access_info(snapshot_id):\n",
    "    uri = f\"{tdr_env_uri}/api/repository/v1/snapshots/{snapshot_id}?include=ACCESS_INFORMATION\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(),\n",
    "               \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved snapshot access information for snapshotID {snapshot_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def load_data(dataset_id, ingest_data):\n",
    "    \"\"\"Load data into TDR\"\"\"\n",
    "    \n",
    "    uri = f\"{tdr_env_uri}/api/repository/v1/datasets/{dataset_id}/ingest\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(),\n",
    "               \"accept\": \"application/json\",\n",
    "               \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    response = requests.post(uri, headers=headers, data=ingest_data)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 202:\n",
    "        return response.text\n",
    "    \n",
    "    print(f\"Successfully retrieved access information for snapshot with snapshotID {snapshot_id}.\")\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "# s.c recoding json function expanded to handle all data types as well as array type columns\n",
    "def create_recoded_json(row_json):\n",
    "    \"\"\"Update dictionary with TDR's dataset relative paths for keys with gs:// paths.\"\"\"\n",
    "\n",
    "    recoded_row_json = dict(row_json)  # update copy instead of original\n",
    "\n",
    "    for key in row_json.keys():  # for column name in row\n",
    "        value = row_json[key]    # get value\n",
    "        if value is not None:  # if value exists (non-empty cell)\n",
    "            if isinstance(value, str):  # and is a string\n",
    "                if value.startswith(\"gs://\"):  # starting with gs://\n",
    "                    relative_tdr_path = value.replace(\"gs://\",\"/\")  # create TDR relative path\n",
    "                    # recode original value/path with expanded request\n",
    "                    # TODO: add in description = id_col + col_name\n",
    "                    recoded_row_json[key] = {\"sourcePath\":value,\n",
    "                                    \"targetPath\":relative_tdr_path,\n",
    "                                    \"mimeType\":\"text/plain\"\n",
    "                                    }\n",
    "                    continue\n",
    "\n",
    "                recoded_row_json_list = []  # instantiate empty list to store recoded values for arrayOf:True cols\n",
    "                if value.startswith(\"[\") and value.endswith(\"]\"):  # if value is an array\n",
    "                    value_list = json.loads(value)  # convert <str> to <liist>\n",
    "\n",
    "                    # check if any of the list values are non-string types\n",
    "                    non_string_list_values = [isinstance(item, str) for item in value_list]\n",
    "                    # if non-string types, add value without recoding\n",
    "                    if not any(non_string_list_values):\n",
    "                        recoded_row_json[key] = value_list\n",
    "                        continue\n",
    "\n",
    "                    # check if any of the list_values are strings that start with gs://\n",
    "                    gs_paths = [item.startswith('gs://') for item in value_list]\n",
    "                    # TODO: any cases where an item in a list is not gs:// should be a user error?\n",
    "                    if any(gs_paths):\n",
    "                        for item in value_list:  # for each item in the array\n",
    "                            relative_tdr_path = item.replace(\"gs://\",\"/\")  # create TDR relative path\n",
    "                            # create the json request for list member\n",
    "                            recoded_list_member = {\"sourcePath\":item,\n",
    "                                                   \"targetPath\":relative_tdr_path,\n",
    "                                                   \"mimeType\":\"text/plain\"\n",
    "                                                   }\n",
    "                            recoded_row_json_list.append(recoded_list_member)  # add json request to list\n",
    "                        recoded_row_json[key] = recoded_row_json_list  # add list of json requests to larger json request\n",
    "                        continue\n",
    "\n",
    "                # if value is string but not a gs:// path or list of gs:// paths\n",
    "                recoded_row_json[key] = value\n",
    "\n",
    "    return recoded_row_json\n",
    "\n",
    "\n",
    "def get_job_status_and_result(job_id):\n",
    "    # first check job status\n",
    "    uri = f\"{tdr_env_uri}/api/repository/v1/jobs/{job_id}\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(),\n",
    "               \"accept\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return response.text\n",
    "    \n",
    "    job_status = response.json()['job_status']\n",
    "    print(f'job_id {job_id} has status {job_status}')\n",
    "    # if job status = done, check job result\n",
    "    if job_status in ['succeeded', 'failed']:\n",
    "        print('retrieving job result')\n",
    "        response = requests.get(uri + \"/result\", headers=headers)\n",
    "        status_code = response.status_code\n",
    "        \n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def get_fq_table(entity_id, table_name, entity_type='dataset'):\n",
    "    \"\"\"Given a datset or snapshot id, table name, and entity type {dataset,snapshot}, retrieve its fully qualified BQ table name\"\"\"\n",
    "    if entity_type == 'dataset':\n",
    "        access_info = get_dataset_access_info(entity_id)\n",
    "    elif entity_type == 'snapshot':\n",
    "        access_info = get_snapshot_access_info(entity_id)\n",
    "\n",
    "    project_id = access_info['accessInformation']['bigQuery']['projectId']\n",
    "    tables = access_info['accessInformation']['bigQuery']['tables']\n",
    "\n",
    "    # pull out desired table\n",
    "    table_fq = None  # fq = fully qualified name, i.e. project.dataset.table\n",
    "    for table_info in tables:\n",
    "        if table_info['name'] == table_name:\n",
    "            table_fq = table_info['qualifiedName'] \n",
    "    \n",
    "    return table_fq\n",
    "\n",
    "\n",
    "def get_single_attribute(fq_bq_table, datarepo_row_id, desired_field):\n",
    "    \"\"\"Performs a BQ lookup of a desired attribute in a specified snapshot or dataset table, for a specified datarepo_row_id\"\"\"\n",
    "    \n",
    "    # create BQ connection\n",
    "    bq = bigquery.Client(gcp_project)\n",
    "    \n",
    "    # execute BQ query\n",
    "#     datarepo_row_id_list_string = \"('\" + \"','\".join(datarepo_row_id_list) + \"')\"\n",
    "    query = f\"\"\"SELECT datarepo_row_id, {desired_field} FROM `{fq_bq_table}` WHERE datarepo_row_id = '{datarepo_row_id}'\"\"\"\n",
    "    executed_query = bq.query(query)\n",
    "    \n",
    "    result = executed_query.result()\n",
    "    \n",
    "    df_result = result.to_dataframe().set_index('datarepo_row_id')\n",
    "    \n",
    "    return df_result[desired_field][datarepo_row_id]\n",
    "\n",
    "\n",
    "def get_all_attributes(fq_bq_table, datarepo_row_id):\n",
    "    \"\"\"Performs a BQ lookup of all attributes in the specified snapshot or dataset table\"\"\"\n",
    "    # create BQ connection\n",
    "    bq = bigquery.Client(gcp_project)\n",
    "    \n",
    "    # execute BQ query\n",
    "    query = f\"\"\"SELECT * FROM `{fq_bq_table}` WHERE datarepo_row_id = '{datarepo_row_id}'\"\"\"\n",
    "    executed_query = bq.query(query)\n",
    "    \n",
    "    result = executed_query.result()\n",
    "    \n",
    "    df_result = result.to_dataframe().set_index('datarepo_row_id')\n",
    "    \n",
    "    json_result = convert_df_to_json(df_result)\n",
    "    \n",
    "    return json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## extract information from the submission whose outputs you're importing back to TDR\n",
    "\n",
    "sub_info = fapi.get_submission(ws_project, ws_name, arrays_submission_id).json()\n",
    "snapshot_id = sub_info['externalEntityInfo']['dataStoreId']\n",
    "print(f\"The Arrays.wdl submission [{arrays_submission_id}] was run with inputs from snapshot ID [{snapshot_id}]\")\n",
    "\n",
    "# find all successful workflows\n",
    "# NOTE: this assumes you're going to have exactly one successful workflow per sample, so...\n",
    "# TODO: build in some edge case handling here\n",
    "workflows = {}  # format will be {datarepo_row_id: workflow_id}\n",
    "for workflow in sub_info['workflows']:\n",
    "    if workflow['status'] != 'Succeeded':\n",
    "        continue\n",
    "    datarepo_row_id = workflow['workflowEntity']['entityName']\n",
    "    workflows[datarepo_row_id] = workflow['workflowId']\n",
    "\n",
    "if verbose:\n",
    "    for key, value in workflows.items():\n",
    "        print(f'datarepo_row_id {key}: workflow_id {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## gather BQ table information\n",
    "\n",
    "# TODO/question: should we just always query the underlying dataset?\n",
    "\n",
    "# for the snapshot (for this sample's info)\n",
    "snapshot_sample_table_fq = get_fq_table(snapshot_id, 'ArraysInputsTable', 'snapshot')\n",
    "\n",
    "if verbose:\n",
    "    print(f'SNAPSHOT sample table: {snapshot_sample_table_fq}')\n",
    "          \n",
    "# and for the underlying dataset (for updates)\n",
    "# retrieve existing data for row to update\n",
    "dataset_sample_table_fq = get_fq_table(dataset_id, 'ArraysInputsTable', 'dataset')\n",
    "\n",
    "if verbose:\n",
    "    print(f'DATASET sample table: {dataset_sample_table_fq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## create a list of recoded jsons where each item represents a workflow's outputs\n",
    "\n",
    "\n",
    "# empty list to collect per-row recoded json requests to ingest dataset\n",
    "all_rows_recoded_data_to_upload = []\n",
    "# empty dictionary to hold single row outputs\n",
    "single_row_data_to_upload = {}\n",
    "# generate timestamp for last_modified_column --> current datetime in UTC\n",
    "last_modified_date = datetime.now(tz=pytz.UTC).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# this is the desired format of data_to_upload (specific format for reblocking wdl):\n",
    "# data_to_upload = {\n",
    "#     \"sample_id\": None,\n",
    "#     \"reblocked_gvcf_path\": None,\n",
    "#     \"reblocked_gvcf_index_path\": None\n",
    "# }\n",
    "\n",
    "for datarepo_row_id, workflow_id in workflows.items():\n",
    "\n",
    "    # retrieve chip_well_barcode from snapshot data\n",
    "    chip_well_barcode = get_single_attribute(snapshot_sample_table_fq, datarepo_row_id, 'chip_well_barcode')\n",
    "    single_row_data_to_upload['chip_well_barcode'] = chip_well_barcode\n",
    "\n",
    "    # get reblocked gvcf & index paths\n",
    "    workflow_outputs_json = fapi.get_workflow_outputs(ws_project, ws_name, arrays_submission_id, workflow_id).json()\n",
    "#     print(f\"workflow_outputs_json {workflow_outputs_json}\" + \"\\n\\n\\n\\n\")\n",
    "\n",
    "    workflow_outputs = workflow_outputs_json['tasks'][workflow_name]['outputs']\n",
    "    \n",
    "    # pull out all the desired workflow outputs (defined in workflow_outputs_dict) and save them in data_to_upload\n",
    "    for output_name, output_value in workflow_outputs.items():\n",
    "        if output_name in workflow_outputs_dict.keys():\n",
    "            single_row_data_to_upload[workflow_outputs_dict[output_name]] = output_value\n",
    "      \n",
    "    # add timestamp to single_row_data_to_upload before recoding for ingestDataset API call\n",
    "    single_row_data_to_upload[\"last_modified_date\"] = last_modified_date\n",
    "    # recode the single row for ingestDataset API call\n",
    "    single_row_recoded_ingest_json = create_recoded_json(single_row_data_to_upload)\n",
    "    # add recoded single row to list of all recoded rows\n",
    "    all_rows_recoded_data_to_upload.append(single_row_recoded_ingest_json)\n",
    "\n",
    "\n",
    "if verbose:\n",
    "    pprint(f\"{all_rows_recoded_data_to_upload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## write the list of recoded rows (list of dictionaries) to a file and copy to workspace bucket\n",
    "\n",
    "loading_json_filename = f\"arrays_sub-{arrays_submission_id}_recoded_ingestDataset.json\"\n",
    "\n",
    "with open(loading_json_filename, 'w') as final_newline_json:\n",
    "    for r in all_rows_recoded_data_to_upload:\n",
    "        json.dump(r, final_newline_json)\n",
    "        final_newline_json.write('\\n')\n",
    "\n",
    "# print(f\"Recoded newline delimited json created: {loading_json_filename}\")\n",
    "\n",
    "# write load json to the workspace bucket\n",
    "control_file_destination = f\"{ws_bucket}/emerge_prod_test_dataset/\"\n",
    "\n",
    "!gsutil cp $loading_json_filename $control_file_destination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"format\": \"json\", \"path\": \"gs://fc-e036248e-067b-4365-8c1e-5eb25103681f/emerge_prod_test_dataset/arrays_sub-b659c03b-78c6-40f4-a385-6f75f31308cd_recoded_ingestDataset.json\", \"table\": \"ArraysOutputsTable\"}\n"
     ]
    }
   ],
   "source": [
    "## create the ingestDataset API json request body\n",
    "\n",
    "load_json = json.dumps({\n",
    "    \"format\": \"json\",\n",
    "    \"path\": f\"{control_file_destination}{loading_json_filename}\",\n",
    "    \"table\": target_table_name\n",
    "})\n",
    "\n",
    "# print(load_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## call the ingestDataset API and return the status of the job using job id\n",
    "load_job_response = load_data(dataset_id, load_json)\n",
    "\n",
    "pprint(load_job_response)\n",
    "\n",
    "load_job_id = load_job_response[\"id\"]\n",
    "get_job_status_and_result(load_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# INDEX CELLS\n",
    "# UNCOMMENT CODE BELOW IF WORKING IN ALPHA ONLY to set FISS config to point to alpha orchestration\n",
    "\n",
    "# alpha_orchestration_url = \"https://firecloud-orchestration.dsde-alpha.broadinstitute.org/api/\"\n",
    "# fcconfig = fccore.config_parse()\n",
    "# fcconfig.set_root_url(alpha_orchestration_url)\n",
    "# fiss_updated_root_url = fccore.config_get_all()[\"root_url\"]  # check config after updating to the alpha url\n",
    "# print(f\"FISS config has the new root_url: {fiss_updated_root_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
