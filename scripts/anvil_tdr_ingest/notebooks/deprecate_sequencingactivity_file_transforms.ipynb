{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace name = anvil_cmg_ingest_resources\n",
      "workspace project = dsp-data-ingest\n",
      "workspace bucket = gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46\n",
      "workspace bucket name = fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46\n"
     ]
    }
   ],
   "source": [
    "## imports and environment variables\n",
    "# imports\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "print(f\"workspace name = {ws_name}\")\n",
    "print(f\"workspace project = {ws_project}\")\n",
    "print(f\"workspace bucket = {ws_bucket}\")\n",
    "print(f\"workspace bucket name = {ws_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Transform functions\n",
    "\n",
    "# Function to convert list represented as string to a list data type\n",
    "def str_list_to_list(in_str, list_delim):\n",
    "    out_list = []\n",
    "    out_list = in_str.split(sep=list_delim)\n",
    "    return out_list\n",
    "\n",
    "# Function to concatenate a string value to each entry in a list (either 'prefix' or 'suffix')\n",
    "def concat_str_to_list(in_str, in_list, delim='_', mode='prefix'):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if mode == 'prefix':\n",
    "            out_list.append(in_str + delim + item)\n",
    "        elif mode == 'suffix':\n",
    "            out_list.append(item + delim + instr)\n",
    "        else:\n",
    "            out_list.append(item)\n",
    "    return out_list\n",
    "\n",
    "# Function to convert non-null values from a list of columns into a list\n",
    "def df_cols_to_list(in_list):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if pd.notnull(item):\n",
    "            out_list.append(item)\n",
    "    return out_list\n",
    "\n",
    "# Function to add value to existing list (or create new list)\n",
    "def add_to_list(curr_value, new_value):\n",
    "    return_list = []\n",
    "    if new_value == None:\n",
    "        if type(curr_value) == list:\n",
    "            return_list = curr_value\n",
    "        else:\n",
    "            return_list.append(curr_value)\n",
    "    elif type(new_value) == list:\n",
    "        if curr_value == None:\n",
    "            return_list = new_value\n",
    "        elif type(curr_value) == list:\n",
    "            return_list = curr_value\n",
    "            for item in new_value:\n",
    "                if item not in curr_value:\n",
    "                    return_list.append(item)      \n",
    "        elif type(curr_value) != list:\n",
    "            return_list.append(curr_value)\n",
    "            for item in new_value:\n",
    "                if item != curr_value:\n",
    "                    return_list.append(item) \n",
    "    elif type(new_value) != list:\n",
    "        if curr_value == None:\n",
    "            return_list.append(new_value)\n",
    "        elif type(curr_value) == list:\n",
    "            return_list = curr_value\n",
    "            if new_value not in curr_value:\n",
    "                return_list.append(new_value)         \n",
    "        elif type(curr_value) != list:\n",
    "            return_list.append(curr_value)\n",
    "            if new_value != curr_value:\n",
    "                return_list.append(new_value)\n",
    "    return return_list\n",
    "\n",
    "# Function to build file reference for file based on a given file name or path\n",
    "def find_file_in_manifest(search_string, file_manifest_dict):\n",
    "    # Loop through file manifest and record fileref_obj_entry where matches are found\n",
    "    fileref_obj = []\n",
    "    match_cnt = 0\n",
    "    for entry in file_manifest_dict:\n",
    "        file_path = ''\n",
    "        if search_string in entry['name']:\n",
    "            match_cnt += 1\n",
    "            fileref_obj.append(entry['file_id'])\n",
    "    \n",
    "    # Return fileref object\n",
    "    if match_cnt == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return fileref_obj\n",
    "\n",
    "#print(find_file_in_manifest('112610'))\n",
    "#print(add_to_list(None, find_file_in_manifest('112610')))\n",
    "\n",
    "\n",
    "# Function to return objects in specified bucket\n",
    "def get_objects_list(bucket_name, dirs_to_exclude=[], dirs_to_include=[]):\n",
    "    \n",
    "    # Collect list of objects/blobs from bucket \n",
    "    obj_list = []\n",
    "    storage_client = storage.Client()\n",
    "    storage_bucket = storage_client.bucket(bucket_name, user_project='dsp-data-ingest')\n",
    "    objects = list(storage_client.list_blobs(storage_bucket))\n",
    "    \n",
    "    # Loop through list of objects and append names to final list based on the roots_to_include and roots_to_exclude variables\n",
    "    for obj in objects:\n",
    "        obj_root = obj.name.split('/')[0]\n",
    "        if len(dirs_to_include) > 0:\n",
    "            for entry in dirs_to_include:\n",
    "                if entry in obj.name:\n",
    "                    obj_list.append(obj.name)\n",
    "        elif len(dirs_to_exclude) > 0:\n",
    "            for entry in dirs_to_exclude:\n",
    "                if entry not in obj.name:\n",
    "                    obj_list.append(obj.name)\n",
    "        else:\n",
    "            obj_list.append(obj.name)\n",
    "    return obj_list\n",
    "\n",
    "# Function to return object metadata\n",
    "def get_object(bucket_name, object_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name, user_project='dsp-data-ingest')\n",
    "    obj = bucket.get_blob(object_name)\n",
    "    return obj\n",
    "\n",
    "def build_file_manifest():\n",
    "\n",
    "    # Set source bucket and directories for data files\n",
    "    data_files_src_bucket = 'fc-secure-da8aff66-e139-4cad-a52d-4359b6f35f36' #PARAMETER\n",
    "    #data_files_src_bucket = 'fc-secure-723963fa-faec-4de1-bf37-12d3c22a417a'\n",
    "    data_files_src_dirs = [] #PARAMETER\n",
    "    data_files_src_dirs_exclude = ['uw_phs000693_gru'] #PARAMETER\n",
    "\n",
    "    # Define record list\n",
    "    record_list = []\n",
    "\n",
    "    # Loop through object list to construct manifest entry for each non-directory object \n",
    "    if data_files_src_bucket == None:\n",
    "        data_files_src_bucket = ws_bucket_name\n",
    "    object_list = get_objects_list(data_files_src_bucket, data_files_src_dirs_exclude, data_files_src_dirs)\n",
    "    for entry in object_list:\n",
    "        if not re.search('/$', entry):\n",
    "            # Collect information for manifest entry record\n",
    "            entry_obj_record = []\n",
    "            entry_obj = get_object(data_files_src_bucket, entry)\n",
    "            entry_obj_uri = 'gs://' + data_files_src_bucket + '/' + entry_obj.name\n",
    "            entry_obj_id_str = entry_obj_uri + entry_obj.md5_hash\n",
    "            entry_obj_id = hashlib.md5(entry_obj_id_str.encode())\n",
    "            entry_obj_file_name = os.path.split(entry_obj.name)[1]\n",
    "            # Construct fileref object\n",
    "            fileref_obj = {}\n",
    "            fileref_obj['sourcePath'] = entry_obj_uri\n",
    "            fileref_obj['targetPath'] = ('/' + entry_obj.name).replace('//', '/')\n",
    "            fileref_obj['description'] = f'Ingest of {entry_obj_uri}'\n",
    "            fileref_obj['mimeType'] = 'text/plain'\n",
    "            # Construct manifest entry record and append to record list\n",
    "            entry_obj_record = [entry_obj_id.hexdigest(), entry_obj_file_name, entry_obj.name, entry_obj_uri, entry_obj.content_type, entry_obj.size, entry_obj.crc32c, entry_obj.md5_hash, fileref_obj]  \n",
    "            record_list.append(entry_obj_record)\n",
    "\n",
    "    # Build manifest dataframe, drop duplicates, and build JSON object\n",
    "    column_list = ['file_id', 'name', 'path', 'uri', 'content_type', 'size_in_bytes', 'crc32c', 'md5_hash', 'file_ref']\n",
    "    global df_file_manifest \n",
    "    df_file_manifest = pd.DataFrame(record_list, columns = column_list)\n",
    "    df_file_manifest.drop_duplicates(['name', 'md5_hash'], keep='first', inplace=True, ignore_index=True)\n",
    "    print(\"File Manifest Complete\")\n",
    "\n",
    "def int_transform_sequencing():\n",
    "    \n",
    "    # Attempt to read source files into data frame, checking for missing files or key fields \n",
    "    try:\n",
    "        src_file = 'sequencing.tsv'\n",
    "        src_file_path = ws_bucket + '/ingest_pipeline/input/metadata/' + src_file\n",
    "        global df_seq\n",
    "        df_seq = pd.read_csv(src_file_path, delimiter = '\\t').rename(columns = {'entity:sequencing_id':'sequencing_id'})\n",
    "        key_fields = ['sequencing_id']\n",
    "        missing_key_fields = []\n",
    "        for item in key_fields:\n",
    "            if item not in df_seq.columns:\n",
    "                missing_key_fields.append(item)\n",
    "        if len(missing_key_fields) > 0:\n",
    "            missing_fields_str = ', '.join(missing_key_fields)\n",
    "            print('ERROR: Key source fields ({fields}) not found in file ({file}).'.format(fields = missing_fields_str, file = src_file))\n",
    "            return\n",
    "    except:\n",
    "        print('ERROR: Source file {src} not found.'.format(src = src_file))\n",
    "        return\n",
    "    \n",
    "    # Check for optional file manifest dataframe and convert to dict if exists\n",
    "    manifest_exists = True\n",
    "    try:\n",
    "        file_manifest = df_file_manifest.to_dict(orient='records')\n",
    "    except:\n",
    "        print('WARNING: File Manifest not found. Will not use.')\n",
    "        manifest_exists = False\n",
    "    \n",
    "    # Transform mapped fields (appending new fields to end of existing DF for now)\n",
    "    if {'sequencing_id'}.issubset(df_seq.columns):\n",
    "        df_seq['sequencingactivity_id'] = df_seq['sequencing_id']\n",
    "    if {'library_prep_kit_method'}.issubset(df_seq.columns):\n",
    "        df_seq['protocol'] = df_seq.apply(lambda x: [x['library_prep_kit_method']] if(pd.notnull(x['library_prep_kit_method'])) else [], axis=1)  \n",
    "    if {'reference_genome_build'}.issubset(df_seq.columns):\n",
    "        df_seq['reference_assembly'] = df_seq.apply(lambda x: [x['reference_genome_build']] if(pd.notnull(x['reference_genome_build'])) else [], axis=1)  \n",
    "    if {'sample_id'}.issubset(df_seq.columns):\n",
    "        df_seq['uses_sample_biosample_id'] = df_seq.apply(lambda x: [x['sample_id']] if(pd.notnull(x['sample_id'])) else [], axis=1) \n",
    "    \n",
    "    # File ref columns\n",
    "    if manifest_exists == True:\n",
    "        df_seq['generated_file_id'] = [[] for _ in range(len(df_seq))]\n",
    "        fileref_columns = ['sequencing_id', 'seq_filename', 'capture_region_bed_file'] #PARAMETER\n",
    "        for item in fileref_columns:\n",
    "            if {item}.issubset(df_seq.columns):\n",
    "                df_seq['generated_file_id'] = df_seq.apply(lambda x: add_to_list(x['generated_file_id'], find_file_in_manifest(x[item], file_manifest)) if(pd.notnull(x[item])) else x['generated_file_id'], axis=1)\n",
    "    \n",
    "    # Limit DF to transformed and passthrough fields\n",
    "    mapped_columns = ['sequencingactivity_id', 'protocol', 'reference_assembly', 'uses_sample_biosample_id', 'generated_file_id']\n",
    "    passthrough_columns = ['analyte_type', 'exome_capture_platform', 'target_insert_size', 'target_depth', 'read_length', 'ancestry_detail', 'number_of_independent_libraries', 'sex', 'submitter_id', 'sequencing_center']\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df_seq.columns:\n",
    "            final_col_list.append(item)\n",
    "    for item in passthrough_columns:\n",
    "        if item in df_seq.columns:\n",
    "            final_col_list.append(item)\n",
    "    df_seq2 = df_seq[final_col_list] # Creating to avoid any cardinality issues when rejoining the passthrough data in the subsequent steps\n",
    "\n",
    "    # Build passthrough string \n",
    "    passthrough_col_list = []\n",
    "    for item in passthrough_columns:\n",
    "        if item in df_seq2.columns:\n",
    "            passthrough_col_list.append(item)\n",
    "    passthrough_col_list.sort()\n",
    "    passthrough_df_seq = df_seq2[passthrough_col_list]\n",
    "    add_data_df_seq = passthrough_df_seq.apply(lambda x: x.to_json(), axis=1).to_frame()\n",
    "    add_data_df_seq.columns = ['additional_data']\n",
    "    \n",
    "    # Merge mapped columns with additional data column to build final df\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df_seq.columns:\n",
    "            final_col_list.append(item)\n",
    "    df_seq_final = df_seq[final_col_list].join(add_data_df_seq)\n",
    "    \n",
    "    # Convert dataframe to new-line delimited JSON and write out to file\n",
    "    destination_dir = 'ingest_pipeline/output/tim_core/metadata'\n",
    "    destination_file = 'sequencingactivity.json'\n",
    "    records_json = df_seq_final.to_json(orient='records') # Converting to JSON string first to replace NaN with nulls\n",
    "    records_list = json.loads(records_json)\n",
    "    records_cnt = len(records_list)\n",
    "\n",
    "    with open(destination_file, 'w') as outfile:\n",
    "        for idx, val in enumerate(records_list):\n",
    "            json.dump(val, outfile) # Adds escape characters to additional_data field --> Not sure it's a problem\n",
    "            if idx < (records_cnt - 1):\n",
    "                outfile.write('\\n')\n",
    "    \n",
    "    # Copy file to workspace bucket\n",
    "    !gsutil cp $destination_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "    \n",
    "def int_transform_file():\n",
    "\n",
    "    # Check for existence of mandatory file manifest dataframe\n",
    "    try:\n",
    "        df_file = df_file_manifest\n",
    "    except:\n",
    "        print('ERROR: File Manifest not found.')\n",
    "        return\n",
    "    \n",
    "    # Check for optional sequencing dataframe and merge in information if it exists\n",
    "    try:\n",
    "        # Explode sequencing dataframe to have one row per file_id\n",
    "        df_seq_explode = df_seq.explode('generated_file_id')\n",
    "\n",
    "        # Determine sequencing columns to pull into file dataframe, then merge in file dataframe\n",
    "        file_transform_columns = ['generated_file_id', 'file_size', 'file_name', 'md5sum', 'datatype']\n",
    "        final_file_transform_col_list = []\n",
    "        for item in file_transform_columns:\n",
    "            if item in df_seq.columns:\n",
    "                final_file_transform_col_list.append(item)\n",
    "        df_file = df_file.merge(df_seq_explode[final_file_transform_col_list], how='left', left_on='file_id', right_on='generated_file_id')\n",
    "    except:\n",
    "        print('WARNING: Optional source file sequencing.tsv not found. File will not be used.')\n",
    "\n",
    "    # Transform mapped fields (appending new fields to end of existing DF for now)\n",
    "    if {'file_id'}.issubset(df_file.columns):\n",
    "        df_file['file_id'] = df_file['file_id']\n",
    "    if {'file_ref'}.issubset(df_file.columns):\n",
    "        df_file['file_ref'] = df_file['file_ref']\n",
    "    if {'file_size'}.issubset(df_file.columns):\n",
    "        df_file['byte_size'] = df_file['file_size']\n",
    "    if {'name'}.issubset(df_file.columns):\n",
    "        df_file['xref'] = df_file.apply(lambda x: [x['path']] if(pd.notnull(x['path'])) else [], axis=1)\n",
    "        \n",
    "    # Limit DF to transformed and passthrough fields\n",
    "    mapped_columns = ['file_id', 'file_ref', 'byte_size', 'xref']\n",
    "    passthrough_columns = ['file_name', 'md5sum', 'datatype']\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df_file.columns:\n",
    "            final_col_list.append(item)\n",
    "    for item in passthrough_columns:\n",
    "        if item in df_file.columns:\n",
    "            final_col_list.append(item)\n",
    "    df_file2 = df_file[final_col_list] # Creating to avoid any cardinality issues when rejoining the passthrough data in the subsequent steps\n",
    "\n",
    "    # Build passthrough string \n",
    "    passthrough_col_list = []\n",
    "    for item in passthrough_columns:\n",
    "        if item in df_file2.columns:\n",
    "            passthrough_col_list.append(item)\n",
    "    passthrough_col_list.sort()\n",
    "    passthrough_df_file = df_file2[passthrough_col_list]\n",
    "    add_data_df_file = passthrough_df_file.apply(lambda x: x.to_json(), axis=1).to_frame()\n",
    "    add_data_df_file.columns = ['additional_data']\n",
    "    \n",
    "    # Merge mapped columns with additional data column to build final df\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df_file.columns:\n",
    "            final_col_list.append(item)\n",
    "    df_file_final = df_file[final_col_list].join(add_data_df_file)\n",
    "    \n",
    "    # Convert dataframe to new-line delimited JSON and write out to file\n",
    "    destination_dir = 'ingest_pipeline/output/tim_core/metadata'\n",
    "    destination_file = 'file.json'\n",
    "    records_json = df_file_final.to_json(orient='records') # Converting to JSON string first to replace NaN with nulls\n",
    "    records_list = json.loads(records_json)\n",
    "    records_cnt = len(records_list)\n",
    "\n",
    "    with open(destination_file, 'w') as outfile:\n",
    "        for idx, val in enumerate(records_list):\n",
    "            json.dump(val, outfile) # Adds escape characters to additional_data field --> Not sure it's a problem\n",
    "            if idx < (records_cnt - 1):\n",
    "                outfile.write('\\n')\n",
    "\n",
    "    # Copy file to workspace bucket\n",
    "    !gsutil cp $destination_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "\n",
    "def transform():\n",
    " \n",
    "    # Build file manifest\n",
    "    print(\"SAFT Building File Manifest\")\n",
    "    build_file_manifest()\n",
    "    \n",
    "    # Execute sequencingactivity transforms\n",
    "    print(\"SAFT Transforming Sequencing\")\n",
    "    int_transform_sequencing()\n",
    "    \n",
    "    # Execute file transforms\n",
    "    print(\"SAFT Transforming File\")\n",
    "    int_transform_file()\n",
    "    \n",
    "    # Delete tsv files from notebook env - they will persist in designated workspace bucket directory\n",
    "    #!rm *.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAFT Building File Manifest\n",
      "File Manifest Complete\n",
      "SAFT Transforming Sequencing\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biosample_transforms.ipynb\t\t  sequencingactivity.json\r\n",
      "diagnosis_transforms.ipynb\t\t  source_files_creation.ipynb\r\n",
      "donor_transforms.ipynb\t\t\t  stdout\r\n",
      "familymember_transforms.ipynb\t\t  tdr_dataset_ingest.ipynb\r\n",
      "ingest_pipeline.ipynb\t\t\t  transform_task.ipynb\r\n",
      "output_data_profiling.ipynb\t\t  utility_set_test_data.ipynb\r\n",
      "sequencingactivity_file_transforms.ipynb  variantcall_transforms.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
