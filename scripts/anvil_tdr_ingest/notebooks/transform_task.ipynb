{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace name = tdr-anvil-ingest-bjt\n",
      "workspace project = dsp-data-ingest\n",
      "workspace bucket = gs://fc-secure-e7856519-5bea-4fec-88ec-dad61673d22f\n"
     ]
    }
   ],
   "source": [
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "from typing import List, Set\n",
    "from enum import Enum\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from typing import Optional, List, Set\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "\n",
    "print(f\"workspace name = {ws_name}\")\n",
    "print(f\"workspace project = {ws_project}\")\n",
    "print(f\"workspace bucket = {ws_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform Classes\n",
    "@dataclass\n",
    "class TransformType(Enum):\n",
    "    # Adds prefix to every item in list in a new column if both columns exist\n",
    "    CONCAT_STR_TO_LIST_PREFIX = \"concat_str_to_list_prefix\"\n",
    "    \n",
    "    # Adds suffix to every item in list in a new column if both columns exist\n",
    "    CONCAT_STR_TO_LIST_SUFFIX = \"concat_str_to_list_suffix\"\n",
    "    \n",
    "    # Combines multiple columns values into a list in a new column\n",
    "    COLS_TO_LIST = \"cols_to_list\"\n",
    "    \n",
    "    # Sets any row with null value in a list column to empty list in a new column\n",
    "    #NA_TO_LIST = \"na_to_list\"\n",
    "    \n",
    "    CUSTOM = \"custom\"\n",
    "\n",
    "@dataclass\n",
    "class TransformerMap:\n",
    "    source_column: str\n",
    "    target_column: str\n",
    "\n",
    "@dataclass\n",
    "class TransformerTransform:\n",
    "    source_columns: List[str]\n",
    "    target_column: str\n",
    "    transform_type: TransformType\n",
    "    custom_code: Optional[str] = None\n",
    "    custom_defs: Optional[str] = None\n",
    "        \n",
    "@dataclass\n",
    "class TransformerSource:\n",
    "    file_name: str\n",
    "    primary_key: str\n",
    "    rename_primary_key: str\n",
    "    join_main: Optional[bool] = None\n",
    "    \n",
    "@dataclass\n",
    "class TransformerRequest:\n",
    "    input_directory: str\n",
    "    output_directory: str\n",
    "    source_files: List[TransformerSource]\n",
    "    destination_table: str\n",
    "    passthrough_cols: List[str]\n",
    "    maps: List[TransformerMap] # Column renames\n",
    "    transforms: List[TransformerTransform]\n",
    "\n",
    "def as_dict(obj):\n",
    "    return {\n",
    "        field: value.value if isinstance(value, Enum) else value for field, value in obj if value is not None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform functions\n",
    "\n",
    "# Function to convert list represented as string to a list data type\n",
    "def str_list_to_list(in_str, list_delim):\n",
    "    out_list = []\n",
    "    out_list = in_str.split(sep=list_delim)\n",
    "    return out_list\n",
    "\n",
    "# Function to concatenate a string value to each entry in a list (either 'prefix' or 'suffix')\n",
    "def concat_str_to_list(in_str, in_list, delim='_', mode='prefix'):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if mode == 'prefix':\n",
    "            out_list.append(in_str + delim + item)\n",
    "        elif mode == 'suffix':\n",
    "            out_list.append(item + delim + instr)\n",
    "        else:\n",
    "            out_list.append(item)\n",
    "    return out_list\n",
    "\n",
    "# Function to convert non-null values from a list of columns into a list\n",
    "def df_cols_to_list(in_list):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if pd.notnull(item):\n",
    "            out_list.append(item)\n",
    "    return out_list\n",
    "\n",
    "def execute_custom_definitions(custom_definitions: str):\n",
    "    exec(custom_definitions)\n",
    "    \n",
    "def execute_custom_code(df: pd.DataFrame, target_column: str, custom_code: str):\n",
    "    # TODO: Code validation\n",
    "    df[target_column] = df.apply(exec(custom_code))\n",
    "    \n",
    "#TODO Verify Cols Exist Before running transform_cols\n",
    "def columns_in_df(df: pd.DataFrame, cols: List[str]) -> List[str]:\n",
    "    return [col for col in cols if col in df]\n",
    "    \n",
    "# Will apply requested transform transform_mapped_fields([\"this_col\", \"that_col\"], \"new_col\", \"transform_type\")\n",
    "def transform_cols(df: pd.DataFrame, tfs: List[TransformerTransform]) -> List[str]:\n",
    "    transformed_cols = []\n",
    "    for tf in tfs:\n",
    "        transform_type = tf.transform_type\n",
    "        print(f\"Applying transform_type {transform_type.value}\")\n",
    "        transformed_cols.append(tf.target_column)\n",
    "        transform_type = tf.transform_type\n",
    "        cols = columns_in_df(df, tf.source_columns)\n",
    "        new_col = tf.target_column\n",
    "        if transform_type.value == \"concat_str_to_list_prefix\":\n",
    "            df[new_col] = df.apply(lambda x: concat_str_to_list(str(x[cols[0]]), str_list_to_list(str(x[cols[1]]), '|'), '_', 'prefix') if(pd.notnull(x[cols[1]])) else [], axis=1)\n",
    "        elif transform_type.value == \"concat_str_to_list_suffix\":\n",
    "            df[new_col] = df.apply(lambda x: concat_str_to_list(str(x[cols[0]]), str_list_to_list(str(x[cols[1]]), '|'), '_', 'prefix') if(pd.notnull(x[cols[1]])) else [], axis=1)\n",
    "        elif transform_type.value == \"cols_to_list\":\n",
    "            df[new_col] = df.apply(lambda x: df_cols_to_list(x[cols]), axis=1)\n",
    "        elif transform_type.value == \"custom\":\n",
    "            exec(f\"\"\"\n",
    "{tf.custom_defs}\n",
    "\"\"\")\n",
    "            df[tf.target_column] = eval(f\"df.apply({tf.custom_code}, axis=1)\")\n",
    "    return transformed_cols\n",
    "\n",
    "# Apply simple transformations and return list of cols transformed\n",
    "def map_cols(df: pd.DataFrame, maps: List[TransformerMap]) -> List[str]:\n",
    "    mapped_columns = []\n",
    "    for tmap in maps:\n",
    "        if tmap.source_column in df.columns:\n",
    "            print(f\"Mapping field {tmap.source_column} to {tmap.target_column}\")\n",
    "            df[tmap.target_column] = df[tmap.source_column]\n",
    "            mapped_columns.append(tmap.target_column)\n",
    "    return mapped_columns\n",
    "\n",
    "# Read source file into a data frame\n",
    "def transform(req: TransformerRequest) -> pd.DataFrame:\n",
    "\n",
    "    source_file = req.source_files[0]\n",
    "    \n",
    "    ## TODO:  Loop through source_files and build dataframes\n",
    "    src_file_path = req.input_directory + '/' + source_file.file_name\n",
    "    \n",
    "    print(f\"transform src_file_path: {src_file_path}\")\n",
    "    \n",
    "    df = pd.read_csv(src_file_path, delimiter = '\\t').rename(columns = {source_file.rename_primary_key:source_file.primary_key})\n",
    "    \n",
    "    # Apply simple col transforms\n",
    "    \n",
    "    # TODO: drop any maps or transforms with missing cols\n",
    "    mapped_cols = map_cols(df, req.maps)\n",
    "    tf_cols = transform_cols(df, req.transforms)\n",
    "    passthrough_cols = columns_in_df(df, req.passthrough_cols)\n",
    "        \n",
    "    print(f\"mapped_cols = {mapped_cols}\")\n",
    "    print(f\"tf_cols = {tf_cols}\")\n",
    "    print(f\"passthrough_cols = {passthrough_cols}\")\n",
    "\n",
    "    final_col_list = mapped_cols + tf_cols\n",
    "    df2 = df[final_col_list] # Creating to avoid any cardinality issues when rejoining the passthrough data in the subsequent steps\n",
    "    \n",
    "    # Build passthrough string \n",
    "    passthrough_cols.sort()\n",
    "    passthrough_df = df[passthrough_cols]\n",
    "    add_data_df = passthrough_df.apply(lambda x: x.to_json(), axis=1).to_frame()\n",
    "    add_data_df.columns = ['additional_data']\n",
    "\n",
    "    # Merge mapped columns with additional data column to build final df\n",
    "    df_final = df2[final_col_list].join(add_data_df)\n",
    "\n",
    "    # Convert dataframe to new-line delimited JSON and write out to file\n",
    "    destination_dir = req.output_directory\n",
    "    destination_file = f\"{req.destination_table}2.json\" #TODO: Remove 2 to write final file\n",
    "    records_json = df_final.to_json(orient='records') # Converting to JSON string first to replace NaN with nulls\n",
    "    records_list = json.loads(records_json)\n",
    "    records_cnt = len(records_list)\n",
    "\n",
    "    with open(destination_file, 'w') as outfile:\n",
    "        for idx, val in enumerate(records_list):\n",
    "            json.dump(val, outfile) # Adds escape characters to additional_data field --> Not sure it's a problem\n",
    "            if idx < (records_cnt - 1):\n",
    "               outfile.write('\\n')\n",
    "            \n",
    "    print(f\"Writing file {destination_file} {ws_bucket}/{destination_dir}\")\n",
    "    \n",
    "    # Copy file to workspace bucket\n",
    "    !gsutil cp $destination_file $ws_bucket/$destination_dir/ 2> stdout    \n",
    "\n",
    "\n",
    "    # Delete tsv files from notebook env - they will persist in designated workspace bucket directory\n",
    "    #!rm $dest_file\n",
    "    \n",
    "    return df_final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
