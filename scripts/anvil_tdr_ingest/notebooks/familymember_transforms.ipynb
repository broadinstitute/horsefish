{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports and environment variables\n",
    "# imports\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "import logging\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform functions\n",
    "\n",
    "# Function to convert list represented as string to a list data type\n",
    "def str_list_to_list(in_str, list_delim):\n",
    "    out_list = []\n",
    "    out_list = in_str.split(sep=list_delim)\n",
    "    return out_list\n",
    "\n",
    "# Function to concatenate a string value to each entry in a list (either 'prefix' or 'suffix')\n",
    "def concat_str_to_list(in_str, in_list, delim='_', mode='prefix'):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if mode == 'prefix':\n",
    "            out_list.append(in_str + delim + item)\n",
    "        elif mode == 'suffix':\n",
    "            out_list.append(item + delim + instr)\n",
    "        else:\n",
    "            out_list.append(item)\n",
    "    return out_list\n",
    "\n",
    "# Function to convert non-null values from a list of columns into a list\n",
    "def df_cols_to_list(in_list):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if pd.notnull(item):\n",
    "            out_list.append(item)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(params):\n",
    "    \n",
    "    # Retrieve parameters of interest\n",
    "    tf_input_dir = params[\"tf_input_dir\"]\n",
    "    tf_output_dir = params[\"tf_output_dir\"]\n",
    "    \n",
    "    # Attempt to read source files into data frame, checking for missing files or key fields \n",
    "    try:\n",
    "        src_file = 'subject.tsv'\n",
    "        src_file_path = ws_bucket + '/' + tf_input_dir + '/' + src_file\n",
    "        df_subject = pd.read_csv(src_file_path, delimiter = '\\t').rename(columns = {'entity:subject_id':'subject_id'})\n",
    "        key_fields = ['subject_id']\n",
    "        missing_key_fields = []\n",
    "        for item in key_fields:\n",
    "            if item not in df_subject.columns:\n",
    "                missing_key_fields.append(item)\n",
    "        if len(missing_key_fields) > 0:\n",
    "            missing_fields_str = ', '.join(missing_key_fields)\n",
    "            logging.error('Key source fields ({fields}) not found in file ({file}).'.format(fields = missing_fields_str, file = src_file))\n",
    "            return\n",
    "    except:\n",
    "        logging.error('Source file {src} not found.'.format(src = src_file))\n",
    "        return\n",
    "    \n",
    "    # Attempt to read in additional optional files\n",
    "    try:\n",
    "        src_file = 'family.tsv'\n",
    "        src_file_path = ws_bucket + '/' + tf_input_dir + '/' + src_file\n",
    "        df_family = pd.read_csv(src_file_path, delimiter = '\\t').rename(columns = {'entity:family_id':'family_id'})\n",
    "        key_fields = ['family_id']\n",
    "        missing_key_fields = []\n",
    "        for item in key_fields:\n",
    "            if item not in df_family.columns:\n",
    "                missing_key_fields.append(item)\n",
    "        if len(missing_key_fields) > 0:\n",
    "            missing_fields_str = ', '.join(missing_key_fields)\n",
    "            logging.warning('Key source fields ({fields}) not found in optional file ({file}). File will not be used'.format(fields = missing_fields_str, file = src_file))\n",
    "            df = df_subject\n",
    "        else:\n",
    "            # Join in fields from optional file if present and no cardinality issues found\n",
    "            if df_family['family_id'].duplicated().sum() > 0:\n",
    "                logging.warning('Field family_id is not unique in optional file family.tsv. File will not be used.')\n",
    "                df = df_subject\n",
    "            else:\n",
    "                df = df_subject.merge(df_family, on='family_id', how='left', suffixes=(None,'_family'))\n",
    "    except:\n",
    "        logging.warning('Optional source file {src} not found. File will not be used.'.format(src = src_file))\n",
    "        df = df_subject\n",
    "    \n",
    "    # Transform mapped fields (appending new fields to end of existing DF for now)\n",
    "    if {'family_id'}.issubset(df.columns):\n",
    "        df['family_id'] = df.apply(lambda x: [x['family_id']] if(pd.notnull(x['family_id'])) else [], axis=1)\n",
    "    if {'subject_id', 'disease_id'}.issubset(df.columns):\n",
    "        df['diagnosis_id'] = df.apply(lambda x: concat_str_to_list(str(x['subject_id']), str_list_to_list(str(x['disease_id']), '|'), '_', 'prefix') if(pd.notnull(x['disease_id'])) else [], axis=1)\n",
    "    if {'ancestry'}.issubset(df.columns):\n",
    "        df['reported_ethnicity'] = df['ancestry']\n",
    "    if {'sex'}.issubset(df.columns):\n",
    "        df['phenotypic_sex'] = df['sex'] \n",
    "        \n",
    "    # Unpivot family member ID columns into rows, where the column has a value\n",
    "    target_unpivot_val_cols = ['subject_id','maternal_id', 'paternal_id', 'twin_id']\n",
    "    target_unpivot_id_cols = ['family_id']\n",
    "    unpivot_val_cols = []\n",
    "    unpivot_id_cols = []\n",
    "    for item in target_unpivot_val_cols:\n",
    "        if item in df.columns:\n",
    "            unpivot_val_cols.append(item)\n",
    "    for item in target_unpivot_id_cols:\n",
    "        if item in df.columns:\n",
    "            unpivot_id_cols.append(item)\n",
    "    df2 = pd.melt(df, id_vars=unpivot_id_cols, value_vars=unpivot_val_cols, var_name = 'src_col', value_name = 'familymember_id').dropna(subset=['familymember_id'])\n",
    "    \n",
    "    # Limit rows to records either source from maternal_id, paternal_id, or twin_id, or records with family_id populated (for subject_ids that haven't already been added)\n",
    "    df_exclude = df2.loc[df2['src_col'].isin(['maternal_id', 'paternal_id', 'twin_id'])]\n",
    "    df3 = df2.loc[(df2['src_col'].isin(['maternal_id', 'paternal_id', 'twin_id']) | (df2['src_col'].isin(['subject_id']) & ~df2['familymember_id'].isin(df_exclude['familymember_id']) & df2['family_id'].str.len() > 0))]\n",
    "    \n",
    "    # Join in additional fields of interest\n",
    "    df4 = df3.merge(df, left_on='familymember_id', right_on='subject_id', how='left', suffixes=('_temp', None))\n",
    "\n",
    "    # View DF\n",
    "    df4.drop_duplicates(['familymember_id'], keep='first', inplace=True, ignore_index=True)\n",
    "    \n",
    "    # Limit DF to transformed and passthrough fields\n",
    "    mapped_columns = ['familymember_id', 'diagnosis_id', 'reported_ethnicity', 'family_id', 'phenotypic_sex']\n",
    "    passthrough_columns = ['proband_relationship', 'consanguinity', 'consanguinity_detail', 'family_history', 'family_onset', 'pedigree_detail', 'pedigree_image']\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df4.columns:\n",
    "            final_col_list.append(item)\n",
    "    for item in passthrough_columns:\n",
    "        if item in df4.columns:\n",
    "            final_col_list.append(item)\n",
    "    df5 = df4[final_col_list] # Creating to avoid any cardinality issues when rejoining the passthrough data in the subsequent steps\n",
    "    \n",
    "    # Build passthrough string \n",
    "    passthrough_col_list = []\n",
    "    for item in passthrough_columns:\n",
    "        if item in df5.columns:\n",
    "            passthrough_col_list.append(item)\n",
    "    passthrough_col_list.sort()\n",
    "    passthrough_df = df5[passthrough_col_list]\n",
    "    add_data_df = passthrough_df.apply(lambda x: x.to_json(), axis=1).to_frame()\n",
    "    add_data_df.columns = ['additional_data']\n",
    "    \n",
    "    # Merge mapped columns with additional data column to build final df\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df5.columns:\n",
    "            final_col_list.append(item)\n",
    "    df_final = df5[final_col_list].join(add_data_df)\n",
    "    \n",
    "    # Convert dataframe to new-line delimited JSON and write out to file\n",
    "    destination_dir = tf_output_dir\n",
    "    destination_file = 'familymember.json'\n",
    "    records_json = df_final.to_json(orient='records') # Converting to JSON string first to replace NaN with nulls\n",
    "    records_list = json.loads(records_json)\n",
    "    records_cnt = len(records_list)\n",
    "\n",
    "    #print(records_cnt)\n",
    "    #print(records_json)\n",
    "    #print(records_list)\n",
    "\n",
    "    with open(destination_file, 'w') as outfile:\n",
    "        for idx, val in enumerate(records_list):\n",
    "            json.dump(val, outfile) # Adds escape characters to additional_data field --> Not sure it's a problem\n",
    "            if idx < (records_cnt - 1):\n",
    "                outfile.write('\\n')\n",
    "\n",
    "    # Copy file to workspace bucket\n",
    "    !gsutil cp $destination_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "    \n",
    "    # Delete tsv files from notebook env - they will persist in designated workspace bucket directory\n",
    "    !rm $destination_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# params[\"tf_input_dir\"] = \"ingest_pipeline/input/metadata\"\n",
    "# params[\"tf_output_dir\"] = \"ingest_pipeline/output/tim_core/metadata\"\n",
    "# transform(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
