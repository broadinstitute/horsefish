{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports and environment variables\n",
    "# imports\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "import logging\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform functions\n",
    "\n",
    "# Function to convert list represented as string to a list data type\n",
    "def str_list_to_list(in_str, list_delim):\n",
    "    out_list = []\n",
    "    out_list = in_str.split(sep=list_delim)\n",
    "    return out_list\n",
    "\n",
    "# Function to concatenate a string value to each entry in a list (either 'prefix' or 'suffix')\n",
    "def concat_str_to_list(in_str, in_list, delim='_', mode='prefix'):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if mode == 'prefix':\n",
    "            out_list.append(in_str + delim + item)\n",
    "        elif mode == 'suffix':\n",
    "            out_list.append(item + delim + instr)\n",
    "        else:\n",
    "            out_list.append(item)\n",
    "    return out_list\n",
    "\n",
    "# Function to convert non-null values from a list of columns into a list\n",
    "def df_cols_to_list(in_list):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if pd.notnull(item):\n",
    "            out_list.append(item)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     57
    ]
   },
   "outputs": [],
   "source": [
    "def transform(params):\n",
    "    \n",
    "    # Retrieve parameters of interest\n",
    "    tf_input_dir = params[\"tf_input_dir\"]\n",
    "    tf_output_dir = params[\"tf_output_dir\"]\n",
    "    \n",
    "    # Attempt to read source files into data frame, checking for missing files or key fields \n",
    "    try:\n",
    "        src_file = 'sample.tsv'\n",
    "        src_file_path = ws_bucket + '/' + tf_input_dir + '/' + src_file\n",
    "        df_sample = pd.read_csv(src_file_path, delimiter = '\\t').rename(columns = {'entity:sample_id':'sample_id'})\n",
    "        key_fields = ['sample_id']\n",
    "        missing_key_fields = []\n",
    "        for item in key_fields:\n",
    "            if item not in df_sample.columns:\n",
    "                missing_key_fields.append(item)\n",
    "        if len(missing_key_fields) > 0:\n",
    "            missing_fields_str = ', '.join(missing_key_fields)\n",
    "            logging.error('Key source fields ({fields}) not found in file ({file}).'.format(fields = missing_fields_str, file = src_file))\n",
    "            return\n",
    "    except:\n",
    "        logging.error('Source file {src} not found.'.format(src = src_file))\n",
    "        return\n",
    "    \n",
    "    # Attempt to read in additional optional files\n",
    "    try:\n",
    "        src_file = 'subject.tsv'\n",
    "        src_file_path = ws_bucket + '/' + tf_input_dir + '/' + src_file\n",
    "        df_subject = pd.read_csv(src_file_path, delimiter = '\\t').rename(columns = {'entity:subject_id':'subject_id'})\n",
    "        key_fields = ['subject_id']\n",
    "        missing_key_fields = []\n",
    "        for item in key_fields:\n",
    "            if item not in df_subject.columns:\n",
    "                missing_key_fields.append(item)\n",
    "        if len(missing_key_fields) > 0:\n",
    "            missing_fields_str = ', '.join(missing_key_fields)\n",
    "            logging.warning('Key source fields ({fields}) not found in optional file ({file}). File will not be used'.format(fields = missing_fields_str, file = src_file))\n",
    "            df = df_sample\n",
    "        else:\n",
    "            # Join in fields from optional file if present and no cardinality issues found\n",
    "            if df_subject['subject_id'].duplicated().sum() > 0:\n",
    "                logging.warning('Field subject_id is not unique in optional file subject.tsv. File will not be used.')\n",
    "                df = df_sample\n",
    "            else:\n",
    "                subject_cols_list = ['subject_id', 'phenotype_group', 'phenotype_description']\n",
    "                subject_cols_list_final = []\n",
    "                for item in subject_cols_list:\n",
    "                    if item in df_subject.columns:\n",
    "                        subject_cols_list_final.append(item)\n",
    "                df = df_sample.merge(df_subject[subject_cols_list_final], on='subject_id', how='left', suffixes=(None,'_sub'))\n",
    "    except:\n",
    "        logging.warning('Optional source file {src} not found. File will not be used.'.format(src = src_file))\n",
    "        df = df_sample\n",
    "\n",
    "    # View DF\n",
    "    df\n",
    "\n",
    "    # Transform mapped fields (appending new fields to end of existing DF for now)\n",
    "    if {'sample_id'}.issubset(df.columns):\n",
    "        df['biosample_id'] = df['sample_id']\n",
    "    if {'subject_id'}.issubset(df.columns):\n",
    "        df['donor_id'] = df.apply(lambda x: [x['subject_id']] if(pd.notnull(x['subject_id'])) else [], axis=1)   \n",
    "    if {'sample_source'}.issubset(df.columns):\n",
    "        df['sample_type'] = df.apply(lambda x: [x['sample_source']] if(pd.notnull(x['sample_source'])) else [], axis=1)   \n",
    "    if {'dbgap_sample_id'}.issubset(df.columns):\n",
    "        df['xref'] = df.apply(lambda x: [x['dbgap_sample_id']] if(pd.notnull(x['dbgap_sample_id'])) else [], axis=1)\n",
    "\n",
    "    # Limit DF to transformed and passthrough fields\n",
    "    mapped_columns = ['biosample_id', 'donor_id', 'sample_type', 'xref']\n",
    "    passthrough_columns = ['phenotype_group', 'phenotype_description', 'tissue_affected_status']\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df.columns:\n",
    "            final_col_list.append(item)\n",
    "    for item in passthrough_columns:\n",
    "        if item in df.columns:\n",
    "            final_col_list.append(item)\n",
    "    df2 = df[final_col_list] # Creating to avoid any cardinality issues when rejoining the passthrough data in the subsequent steps\n",
    "\n",
    "    # Build passthrough string \n",
    "    passthrough_col_list = []\n",
    "    for item in passthrough_columns:\n",
    "        if item in df2.columns:\n",
    "            passthrough_col_list.append(item)\n",
    "    passthrough_col_list.sort()\n",
    "    passthrough_df = df2[passthrough_col_list]\n",
    "    add_data_df = passthrough_df.apply(lambda x: x.to_json(), axis=1).to_frame()\n",
    "    add_data_df.columns = ['additional_data']\n",
    "\n",
    "    # Merge mapped columns with additional data column to build final df\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df.columns:\n",
    "            final_col_list.append(item)\n",
    "    df_final = df2[final_col_list].join(add_data_df)\n",
    "\n",
    "    # View DF\n",
    "    #df_final\n",
    "\n",
    "    # Convert dataframe to new-line delimited JSON and write out to file\n",
    "    destination_dir = tf_output_dir\n",
    "    destination_file = 'biosample.json'\n",
    "    records_json = df_final.to_json(orient='records') # Converting to JSON string first to replace NaN with nulls\n",
    "    records_list = json.loads(records_json)\n",
    "    records_cnt = len(records_list)\n",
    "\n",
    "    #print(records_cnt)\n",
    "    #print(records_json)\n",
    "    #print(records_list)\n",
    "\n",
    "    with open(destination_file, 'w') as outfile:\n",
    "        for idx, val in enumerate(records_list):\n",
    "            json.dump(val, outfile) # Adds escape characters to additional_data field --> Not sure it's a problem\n",
    "            if idx < (records_cnt - 1):\n",
    "                outfile.write('\\n')\n",
    "\n",
    "    # Copy file to workspace bucket\n",
    "    !gsutil cp $destination_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "\n",
    "    # Delete tsv files from notebook env - they will persist in designated workspace bucket directory\n",
    "    !rm $destination_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# params[\"tf_input_dir\"] = \"ingest_pipeline/input/metadata\"\n",
    "# params[\"tf_output_dir\"] = \"ingest_pipeline/output/tim_core/metadata\"\n",
    "# transform(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
