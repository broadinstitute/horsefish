{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## imports and environment variables\n",
    "# imports\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import re\n",
    "import hashlib\n",
    "import logging\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def profile_data(params):\n",
    "    \n",
    "    # Retrieve parameters of interest\n",
    "    bq_project = params[\"bq_project\"]\n",
    "    bq_schema = params[\"bq_schema\"]\n",
    "    tdr_schema_file = params[\"tdr_schema_file\"]\n",
    "    val_output_dir = params[\"val_output_dir\"]\n",
    "    val_output_file = bq_schema + \"_metric_results.csv\"\n",
    "\n",
    "    # Read schema object from GCS into a dictionary\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "    schema_blob = bucket.blob(tdr_schema_file)\n",
    "    schema_dict = json.loads(schema_blob.download_as_string(client=None))\n",
    "\n",
    "    # Parse schema into something more useful for building queries: A table set, a list of fields, and a list of array fields\n",
    "    table_set = set()\n",
    "    array_field_set = set()\n",
    "    field_list = []\n",
    "    for table_entry in schema_dict['tables']:\n",
    "        table_set.add(table_entry['name'])\n",
    "        for column_entry in table_entry['columns']:\n",
    "            field_dict = {}\n",
    "            field_dict['table'] = table_entry['name']\n",
    "            field_dict['column'] = column_entry['name']\n",
    "            field_dict['datatype'] = column_entry['datatype']\n",
    "            field_dict['is_array'] = column_entry['array_of']\n",
    "            if column_entry['name'] in table_entry['primaryKey']:\n",
    "                field_dict['is_primary_key'] = True\n",
    "            else:\n",
    "                field_dict['is_primary_key'] = False\n",
    "            joins_to_list = []\n",
    "            for relation_entry in schema_dict['relationships']:\n",
    "                joins_to_dict = {}\n",
    "                if relation_entry['from']['table'] == table_entry['name'] and relation_entry['from']['column'] == column_entry['name']:\n",
    "                    joins_to_dict['table'] = relation_entry['to']['table']\n",
    "                    joins_to_dict['column'] = relation_entry['to']['column']\n",
    "                    joins_to_list.append(joins_to_dict)\n",
    "            field_dict['joins_to'] = joins_to_list\n",
    "            field_list.append(field_dict)\n",
    "            if column_entry['array_of'] == True:\n",
    "                array_field_set.add(table_entry['name'] + '.' + column_entry['name'])\n",
    "\n",
    "    # Collect all of the incoming references to files (to build file orphan checks)\n",
    "    file_join_list = []\n",
    "    for relation_entry in schema_dict['relationships']:\n",
    "        inner_list = []\n",
    "        if relation_entry['to']['table'] == 'file' and relation_entry['to']['column'] == 'file_id':\n",
    "            inner_list.append(relation_entry['from']['table'])\n",
    "            inner_list.append(relation_entry['from']['column'])\n",
    "            file_join_list.append(inner_list)\n",
    "    \n",
    "    # Initialize metric collect from BigQuery and create dataframe to store results \n",
    "    client = bigquery.Client()\n",
    "    df = pd.DataFrame(columns = ['metric_type', 'source_table', 'source_column', 'metric', 'n', 'd', 'r'])\n",
    "    \n",
    "    \n",
    "    ## Build and execute table-level queries\n",
    "    logging.info(\"Building and executing table-level queries.\")\n",
    "    # Loop through tables in the table set and pull record counts\n",
    "    for table_entry in table_set:\n",
    "\n",
    "        # Construct the query\n",
    "        query = \"\"\"SELECT 'Summary Stats' AS metric_type, '{table}' AS source_table, 'All' AS source_column, \n",
    "                   'Count of records in table' AS metric, \n",
    "                   COUNT(*) AS n, null AS d, null AS r \n",
    "                   FROM `{project}.{schema}.{table}`\"\"\".format(project = bq_project, schema = bq_schema, table = table_entry)\n",
    "\n",
    "        # Execute the query and append results to dataframe\n",
    "        df = df.append(client.query(query).result().to_dataframe())\n",
    "    \n",
    "    \n",
    "    ## Build and execute column-level queries\n",
    "    logging.info(\"Building and executing column-level queries.\")\n",
    "    # Loop through columns and pull null counts and distinct value counts\n",
    "    for column_entry in field_list:\n",
    "    \n",
    "        # Set parameters for null count and distinct count queries\n",
    "        table_name = column_entry['table']\n",
    "        col_name = column_entry['column']\n",
    "        if column_entry['is_array'] == True:\n",
    "            null_case_statement = 'array_length({col}) = 0'.format(col = col_name)\n",
    "            distinct_case_statement = \"(select string_agg(cast(val as string), ',') from unnest({col}) val)\".format(col = col_name)\n",
    "        else:\n",
    "            null_case_statement = '{col} is null'.format(col = col_name)\n",
    "            distinct_case_statement = col_name\n",
    "\n",
    "        # Construct the null count query\n",
    "        null_query = \"\"\"SELECT 'Summary Stats' AS metric_type, '{table}' AS source_table, '{col}' AS source_column, \n",
    "                   'Count of nulls or empty lists in column' AS metric, \n",
    "                   SUM(CASE WHEN {null_case} THEN 1 ELSE 0 END) AS n, \n",
    "                   COUNT(*) AS d, \n",
    "                   CASE WHEN COUNT(*) > 0 THEN SUM(CASE WHEN {null_case} THEN 1 ELSE 0 END)/COUNT(*) END AS r \n",
    "                   FROM `{project}.{schema}.{table}`\"\"\".format(project = bq_project, schema = bq_schema, table = table_name, col = col_name, null_case = null_case_statement)\n",
    "\n",
    "        # Execute the null count query and append results to dataframe\n",
    "        df = df.append(client.query(null_query).result().to_dataframe())\n",
    "        #print(null_query)\n",
    "\n",
    "        # Construct the distinct count query\n",
    "        distinct_query = \"\"\"SELECT 'Summary Stats' AS metric_type, '{table}' AS source_table, '{col}' AS source_column, \n",
    "                   'Count of distinct values in column' AS metric, \n",
    "                   COUNT(DISTINCT {distinct_case}) AS n, \n",
    "                   COUNT(*) AS d, \n",
    "                   CASE WHEN COUNT(*) > 0 THEN COUNT(DISTINCT {distinct_case})/COUNT(*) END AS r \n",
    "                   FROM `{project}.{schema}.{table}`\"\"\".format(project = bq_project, schema = bq_schema, table = table_name, col = col_name, distinct_case = distinct_case_statement)\n",
    "\n",
    "        # Execute the distinct count query and append results to dataframe\n",
    "        df = df.append(client.query(distinct_query).result().to_dataframe())\n",
    "        #print(distinct_query)\n",
    "\n",
    "        # Loop through join fields (if any) and build referential integrity queries \n",
    "        for join_entry in column_entry['joins_to']:\n",
    "\n",
    "            # Set parameters for referential integrity queries\n",
    "            target_table = join_entry['table']\n",
    "            target_col = join_entry['column']\n",
    "            target_table_col = target_table + '.' + target_col\n",
    "            if column_entry['is_array'] == True:\n",
    "                src_col_name = '{col}_unnest'.format(col = col_name)\n",
    "                from_statement = '(select * from `{project}.{schema}.{table}` t left join unnest(t.{col}) as {unnest_col}) src'.format(project = bq_project, schema = bq_schema, table = table_name, col = col_name, unnest_col = src_col_name)\n",
    "                where_statement = 'array_length(src.{col}) > 0'.format(col = col_name)\n",
    "            else:\n",
    "                src_col_name = col_name\n",
    "                from_statement = '`{project}.{schema}.{table}` src'.format(project = bq_project, schema = bq_schema, table = table_name)\n",
    "                where_statement = 'src.{col} is not null'.format(col = col_name)\n",
    "            if target_table_col in array_field_set:\n",
    "                tar_col_name = '{col}_unnest'.format(col = target_col)\n",
    "                join_statement = '(select * from `{project}.{schema}.{table}` t left join unnest(t.{col}) as {unnest_col}) tar'.format(project = bq_project, schema = bq_schema, table = target_table, col = target_col, unnest_col = tar_col_name)\n",
    "            else:\n",
    "                tar_col_name = target_col\n",
    "                join_statement = '`{project}.{schema}.{table}` tar'.format(project = bq_project, schema = bq_schema, table = target_table)\n",
    "\n",
    "            # Construct the referential integrity query\n",
    "            ref_int_query = \"\"\"SELECT 'Referential Integrity' AS metric_type, '{table}' AS source_table, '{col}' AS source_column, \n",
    "                   'Count of non-null rows that do not fully join to {target}' AS metric, \n",
    "                   COUNT(DISTINCT CASE WHEN tar.datarepo_row_id IS NULL THEN src.datarepo_row_id END) AS n, \n",
    "                   COUNT(DISTINCT src.datarepo_row_id) AS d, \n",
    "                   CASE WHEN COUNT(DISTINCT src.datarepo_row_id) > 0 THEN COUNT(DISTINCT CASE WHEN tar.datarepo_row_id IS NULL THEN src.datarepo_row_id END)/COUNT(DISTINCT src.datarepo_row_id) END AS r\n",
    "                   FROM {frm}\n",
    "                   LEFT JOIN {join}\n",
    "                   ON src.{src_col} = tar.{tar_col}\n",
    "                   WHERE {where}\"\"\".format(project = bq_project, schema = bq_schema, table = table_name, col = col_name, target = target_table_col, frm = from_statement, join = join_statement, src_col = src_col_name, tar_col = tar_col_name, where = where_statement)\n",
    "\n",
    "            # Execute the referential integrity query and append results to dataframe\n",
    "            df = df.append(client.query(ref_int_query).result().to_dataframe())\n",
    "            #print(ref_int_query)\n",
    "    \n",
    "    \n",
    "    ## Build and execute file orphan query\n",
    "    logging.info(\"Building and executing orphan file queries.\")\n",
    "    # Construct file foreign keys CTE query\n",
    "    counter = 0\n",
    "    cte_query = 'WITH file_fks AS ('\n",
    "    for entry in file_join_list:\n",
    "        cte_query_segment = ''\n",
    "        counter += 1\n",
    "        source_table = entry[0]\n",
    "        source_column = entry[1]\n",
    "        source_table_col = entry[0] + '.' + entry[1]\n",
    "        if counter > 1:\n",
    "            cte_query_segment = 'UNION ALL '\n",
    "        if source_table_col in array_field_set:\n",
    "            cte_query_segment = cte_query_segment + 'SELECT DISTINCT file_id FROM `{project}.{schema}.{table}` CROSS JOIN UNNEST({col}) AS file_id'.format(project = bq_project, schema = bq_schema, table = source_table, col = source_column)\n",
    "        else:\n",
    "            cte_query_segment = cte_query_segment + 'SELECT DISTINCT {col} as file_id FROM `{project}.{schema}.{table}`'.format(project = bq_project, schema = bq_schema, table = source_table, col = source_column)\n",
    "        cte_query = cte_query + cte_query_segment + ' '\n",
    "    cte_query = cte_query + ')'\n",
    "\n",
    "    # Construct orphaned files query\n",
    "    orphaned_file_query = \"\"\"{cte} SELECT 'Orphaned Files' As metric_type, 'file' AS source_table, 'file_id' AS source_column, \n",
    "                          'Count of file_ids not referenced by another table' AS metric,\n",
    "                          COUNT(DISTINCT CASE WHEN tar.file_id IS NULL THEN src.file_id END) AS n,\n",
    "                          COUNT(DISTINCT src.file_id) AS d,\n",
    "                          CASE WHEN COUNT(DISTINCT src.file_id) > 0 THEN COUNT(DISTINCT CASE WHEN tar.file_id IS NULL THEN src.file_id END)/COUNT(DISTINCT src.file_id) END AS r\n",
    "                          FROM `{project}.{schema}.file` src LEFT JOIN file_fks tar ON src.file_id = tar.file_id\"\"\".format(cte = cte_query, project = bq_project, schema = bq_schema)\n",
    "\n",
    "    # Execute the orphaned files query and append results to dataframe\n",
    "    df = df.append(client.query(orphaned_file_query).result().to_dataframe())\n",
    "    \n",
    "    ## Write metrics results dataframe out to CSV\n",
    "    logging.info(f\"Writing out results to {val_output_dir}/{val_output_file}.\")\n",
    "    df_final = df.fillna(0)\n",
    "    destination_dir = val_output_dir\n",
    "    destination_file = val_output_file\n",
    "    df_final.to_csv(destination_file, index=False)\n",
    "\n",
    "    # Copy file to workspace bucket\n",
    "    !gsutil cp $destination_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "\n",
    "    # Remove file from notebook environment\n",
    "    !rm $destination_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/10/2022 04:11:58 PM - INFO: Building and executing table-level queries.\n",
      "05/10/2022 04:12:19 PM - INFO: Building and executing column-level queries.\n",
      "05/10/2022 04:27:41 PM - INFO: Building and executing orphan file queries.\n",
      "05/10/2022 04:27:44 PM - INFO: Writing out results to ingest_pipeline/output/tim_core/validation/datarepo_tdr_anvil_ingest_bjt_metric_results.csv.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# params[\"val_output_dir\"] = \"ingest_pipeline/output/tim_core/validation\"\n",
    "# params[\"tdr_schema_file\"] = \"ingest_pipeline/output/tim_core/schema/tdr_schema_object.json\"\n",
    "# params[\"bq_project\"] = \"datarepo-7949025c\"\n",
    "# params[\"bq_schema\"] = \"datarepo_tdr_anvil_ingest_bjt\"\n",
    "# profile_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}