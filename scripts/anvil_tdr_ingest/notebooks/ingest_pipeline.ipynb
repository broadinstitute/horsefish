{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install import_ipynb polling2 dataclasses_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from tdr_dataset_ingest.ipynb\n",
      "workspace name = anvil_cmg_ingest_resources\n",
      "workspace project = dsp-data-ingest\n",
      "workspace bucket = gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46\n",
      "importing Jupyter notebook from transform_task.ipynb\n",
      "workspace name = anvil_cmg_ingest_resources\n",
      "workspace project = dsp-data-ingest\n",
      "workspace bucket = gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46\n",
      "importing Jupyter notebook from source_files_creation.ipynb\n",
      "workspace name = anvil_cmg_ingest_resources\n",
      "workspace project = dsp-data-ingest\n",
      "workspace bucket = gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46\n",
      "importing Jupyter notebook from build_file_manifest.ipynb\n",
      "importing Jupyter notebook from biosample_transforms.ipynb\n",
      "importing Jupyter notebook from sequencingactivity_transforms.ipynb\n",
      "importing Jupyter notebook from file_transforms.ipynb\n",
      "importing Jupyter notebook from familymember_transforms.ipynb\n",
      "importing Jupyter notebook from diagnosis_transforms.ipynb\n",
      "importing Jupyter notebook from donor_transforms.ipynb\n",
      "importing Jupyter notebook from variantcall_transforms.ipynb\n",
      "importing Jupyter notebook from output_data_profiling.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import os\n",
    "import pandas as pd\n",
    "from tdr_dataset_ingest import *\n",
    "from transform_task import *\n",
    "from source_files_creation import *\n",
    "import build_file_manifest as bfm\n",
    "import biosample_transforms as bst\n",
    "import sequencingactivity_transforms as sat\n",
    "import file_transforms as ft\n",
    "import familymember_transforms as fmt\n",
    "import diagnosis_transforms as dt\n",
    "import donor_transforms as dont\n",
    "import variantcall_transforms as vct\n",
    "import output_data_profiling as odp\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import requests\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Run Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should_create_dataset = False\n",
      "tf_input_dir = ingest_pipeline/input/metadata\n",
      "tf_output_dir = ingest_pipeline/output/tim_core/metadata\n",
      "dataset_key = ingest_pipeline/output/tim_core/schema/tdr_schema_object.json\n",
      "bucket_name = fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46\n"
     ]
    }
   ],
   "source": [
    "# Has dataset already been created? False if so.  The pipeline will then get the dataset id using the dataset_name value\n",
    "should_create_dataset = False\n",
    "\n",
    "# Can the creation of source files be skipped? True if so.\n",
    "skip_source_file_creation = False\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# Transform Variables\n",
    "params = {}\n",
    "params[\"tf_input_dir\"] = \"ingest_pipeline/input/metadata\"\n",
    "params[\"tf_output_dir\"] = \"ingest_pipeline/output/tim_core/metadata\"\n",
    "params[\"val_output_dir\"] = \"ingest_pipeline/output/tim_core/validation\"\n",
    "params[\"tdr_schema_file\"] = \"ingest_pipeline/output/tim_core/schema/tdr_schema_object.json\"\n",
    "params[\"data_files_src_bucket\"] = \"fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46\"\n",
    "params[\"data_files_src_dirs\"] = []  # Leave empty to include all\n",
    "params[\"data_files_src_dirs_exclude\"] = [] \n",
    "params[\"fileref_columns\"] = [\"sequencing_id\", \"seq_filename\", \"capture_region_bed_file\"]\n",
    "\n",
    "# List of tuples (transform file, table_name (target table/target tsv name minus type))\n",
    "transformables = [(\"donor_transforms\", \"donor\"),\n",
    "                  (\"familymember_transforms\", \"familymember\"),\n",
    "                  (\"diagnosis_transforms\", \"diagnosis\"),\n",
    "                  (\"biosample_transforms\", \"biosample\"),\n",
    "                  (\"sequencingactivity_transforms\", \"sequencingactivity\"), \n",
    "                  (\"file_transforms\", \"file\"),\n",
    "                  (None, \"variantcall\"),\n",
    "                  (None, \"dataset\"),\n",
    "                  (None, \"project\")]\n",
    "\n",
    "# Ingest Variables\n",
    "dataset_name = ws_name.replace('-', '_') + '_TEST'\n",
    "profile_id = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "dataset_key = params[\"tdr_schema_file\"]\n",
    "parsed_bucket = urlparse(ws_bucket)\n",
    "bucket_name = parsed_bucket.netloc\n",
    "\n",
    "## Setup Google Creds\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "\n",
    "## Setup TDR Client\n",
    "client = TDRClient(\"data.terra.bio\", creds.token)\n",
    "\n",
    "print(f\"should_create_dataset = {should_create_dataset}\")\n",
    "print(\"tf_input_dir = {in_dir}\".format(in_dir = params[\"tf_input_dir\"]))\n",
    "print(\"tf_output_dir = {out_dir}\".format(out_dir = params[\"tf_output_dir\"]))\n",
    "print(f\"dataset_key = {dataset_key}\")\n",
    "print(f\"bucket_name = {bucket_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token():\n",
    "    \"\"\"Get access token.\"\"\"\n",
    "\n",
    "    scopes = [\"https://www.googleapis.com/auth/userinfo.profile\", \"https://www.googleapis.com/auth/userinfo.email\"]\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    credentials = credentials.create_scoped(scopes)\n",
    "\n",
    "    return credentials.get_access_token().access_token\n",
    "\n",
    "def file_exists(key: str) -> bool:\n",
    "    logging.info(f\"Checking file {key}\")\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    return storage.Blob(bucket=bucket, name=key).exists(storage_client)\n",
    "\n",
    "def run_ingest(dataset_id: str):    \n",
    "    # Loop through tfs to get the table files and ingest\n",
    "    for tf in transformables:\n",
    "        table = tf[1]\n",
    "        tf_output_dir = params[\"tf_output_dir\"]\n",
    "        key = f\"{tf_output_dir}/{table}.json\"\n",
    "        metadata_path = f\"{ws_bucket}/{key}\"\n",
    "        \n",
    "        if file_exists(key):\n",
    "            logging.info(f\"Running ingest: {table}\")\n",
    "            ingest_req = TDRIngestRequest(table=table, format=\"json\", path=metadata_path, resolve_existing_files=True, updateStrategy=UpdateEnum.REPLACE)    \n",
    "            ingest_resp = client.ingest(dataset_id, ingest_req)\n",
    "        \n",
    "            poll_resp = client.poll_job_status(ingest_resp.id)\n",
    "        \n",
    "            logging.info(f\"Ingest {table} response {poll_resp}\")\n",
    "        else:\n",
    "            logging.warning(f\"Metadata file does not exist.  Skipping: {table}\")\n",
    "            continue\n",
    "            \n",
    "def run_dataset() -> str:\n",
    "    logging.info(f\"Running create dataset {should_create_dataset}\")\n",
    "    \n",
    "    dataset_id = \"\"\n",
    "    \n",
    "    # Only create the dataset if needed\n",
    "    if should_create_dataset == True:\n",
    "        logging.info(\"Creating dataset\")\n",
    "        # Grab Dataset config from storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        dataset_blob = bucket.blob(dataset_key)\n",
    "        \n",
    "        # Grab phsId from the workspace attributes\n",
    "        w_att = get_workspace_attributes(ws_project, ws_name)\n",
    "        phs_id = get_workspace_phs_id(w_att)\n",
    "\n",
    "        ## Build Validated Request\n",
    "        schema = SchemaModel(**json.loads(dataset_blob.download_as_string(client=None)))        \n",
    "        dataset_req = TDRDatasetRequest(name=dataset_name, defaultProfileId=profile_id, schema=schema, experimentalSelfHosted=True, phsId=phs_id)\n",
    "\n",
    "        resp = client.create_dataset(dataset_req)\n",
    "\n",
    "        poll_resp = client.poll_job_status(resp.id)\n",
    "        \n",
    "    logging.info(\"Getting dataset\")\n",
    "    \n",
    "    # Now grab the dataset by name   \n",
    "    dataset_resp = client.get_dataset_by_name(TDRDatasetSearchRequest(filter=dataset_name))\n",
    "    \n",
    "    logging.info(f\"dataset_resp = {dataset_resp}\")\n",
    "    \n",
    "    # Grab first item from dataset_resp\n",
    "    dataset_id = dataset_resp.items[0][\"id\"]\n",
    "    logging.info(f\"dataset_id = {dataset_id}\")\n",
    "    return dataset_id\n",
    "\n",
    "def get_dataset_details(id: str) -> TDRDatasetDetail:\n",
    "    return client.get_dataset_details(id)\n",
    "\n",
    "def run_transform(params):\n",
    "    for tf in transformables:\n",
    "        logging.info(f\"Running {tf[0]}\")\n",
    "        if tf[0] == None:\n",
    "            continue\n",
    "        elif tf[0] == \"biosample_transforms\":\n",
    "            bst.transform(params)\n",
    "        elif tf[0] == \"familymember_transforms\":\n",
    "            fmt.transform(params)\n",
    "        elif tf[0] == \"diagnosis_transforms\":\n",
    "             dt.transform(params)\n",
    "        elif tf[0] == \"donor_transforms\":\n",
    "            dont.transform(params)\n",
    "        elif tf[0] == \"sequencingactivity_transforms\":\n",
    "            sat.transform(params)\n",
    "        elif tf[0] == \"file_transforms\":\n",
    "            ft.transform(params)\n",
    "        elif tf[0] == \"variantcall_transforms\":\n",
    "            logging.warning(\"Don't forget to test varicantcall_transforms when available!\")\n",
    "        elif tf[0] == \"project\":\n",
    "            logging.warning(\"Don't forget to test project transforms when available!\")\n",
    "        elif tf[0] == \"dataset\":\n",
    "            logging.warning(\"Don't forget to test dataset transforms when available!\")\n",
    "    \n",
    "def run_source_file_creation():\n",
    "    if skip_source_file_creation == True:\n",
    "        logging.info(\"Skipping source file creation\")\n",
    "    else:\n",
    "        logging.info(\"Running source file creation\")\n",
    "        create_source_files()\n",
    "\n",
    "def run_build_file_manifest(params):\n",
    "    logging.info(\"Building data file manifest\")\n",
    "    manifest = bfm.build_manifest(params)\n",
    "    return manifest\n",
    "    \n",
    "def run_output_data_profiling(params, dataset_id):\n",
    "    logging.info(\"Running output data profiling\")\n",
    "    \n",
    "    # Get dataset details to add to params dict\n",
    "    dataset_details = (get_dataset_details(dataset_id)).accessInformation.bigQuery\n",
    "    dataset_bq_name = dataset_details.datasetName \n",
    "    dataset_bq_project_id = dataset_details.projectId\n",
    "    params[\"bq_project\"] = dataset_bq_project_id\n",
    "    params[\"bq_schema\"] = dataset_bq_name\n",
    "    \n",
    "    # Profile data\n",
    "    odp.profile_data(params)\n",
    "\n",
    "def share_dataset(dataset_id):\n",
    "    uri = f\"https://data.terra.bio/api/repository/v1/datasets/{dataset_id}/policies/steward/members\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer \" + get_access_token(),\n",
    "               \"accept\": \"application/json\",\n",
    "               \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    email = json.dumps({\"email\":\"prod-dsp-data-ingest@firecloud.org\"})\n",
    "    request = requests.post(uri, headers=headers, data=email)\n",
    "    \n",
    "    logging.info(f\"Adding {email} as Steward to dataset.\")\n",
    "    \n",
    "def run_pipeline():\n",
    "    # Step 1: Create Source Files\n",
    "    run_source_file_creation()\n",
    "    \n",
    "    # Step 2: Build File Manifest\n",
    "    file_manifest = run_build_file_manifest(params)\n",
    "    params[\"file_manifest\"] = file_manifest\n",
    "    \n",
    "    # Step 3: Transform Structured Data\n",
    "    run_transform(params)\n",
    "    \n",
    "    # Step 4: Create or Retrieve Dataset\n",
    "    dataset_id = run_dataset()\n",
    "    share_dataset(dataset_id)\n",
    "\n",
    "    # Step 5: Ingest Data\n",
    "    run_ingest(dataset_id)\n",
    "    \n",
    "    # Step 6: Share Dataset\n",
    "    share_dataset(dataset_id)\n",
    "    \n",
    "    # Step 7: Profile Output Data \n",
    "    run_output_data_profiling(params, dataset_id) \n",
    "    \n",
    "    logging.info(\"Yay! The pipeline has completed!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2022 02:11:05 PM - INFO: Skipping source file creation\n",
      "05/11/2022 02:11:05 PM - INFO: Building data file manifest\n",
      "05/11/2022 02:11:08 PM - INFO: Running donor_transforms\n",
      "05/11/2022 02:11:14 PM - INFO: Running familymember_transforms\n",
      "05/11/2022 02:11:18 PM - INFO: Running diagnosis_transforms\n",
      "05/11/2022 02:11:22 PM - INFO: Running biosample_transforms\n",
      "05/11/2022 02:11:26 PM - INFO: Running sequencingactivity_transforms\n",
      "05/11/2022 02:11:29 PM - INFO: Running file_transforms\n",
      "05/11/2022 02:11:33 PM - INFO: Running None\n",
      "05/11/2022 02:11:33 PM - INFO: Running None\n",
      "05/11/2022 02:11:33 PM - INFO: Running None\n",
      "05/11/2022 02:11:33 PM - INFO: Running create dataset False\n",
      "05/11/2022 02:11:33 PM - INFO: Getting dataset\n",
      "05/11/2022 02:11:33 PM - INFO: dataset_resp = TDRDataset(total=295, filteredTotal=1, items=[{'id': 'fce017a3-1fc0-4cd6-9638-6a1f8dcd1c09', 'name': 'anvil_cmg_ingest_resources_TEST', 'description': None, 'defaultProfileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'createdDate': '2022-05-10T15:22:37.882211Z', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-3ac3b0b5', 'storageAccount': None, 'phsId': 'phs999999', 'selfHosted': True}], roleMap={'fce017a3-1fc0-4cd6-9638-6a1f8dcd1c09': ['steward', 'custodian']})\n",
      "05/11/2022 02:11:33 PM - INFO: dataset_id = fce017a3-1fc0-4cd6-9638-6a1f8dcd1c09\n",
      "05/11/2022 02:11:34 PM - INFO: Adding {\"email\": \"prod-dsp-data-ingest@firecloud.org\"} as Steward to dataset.\n",
      "05/11/2022 02:11:34 PM - INFO: Checking file ingest_pipeline/output/tim_core/metadata/donor.json\n",
      "05/11/2022 02:11:34 PM - INFO: Running ingest: donor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'json': {'table': 'donor', 'format': 'json', 'path': 'gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/donor.json', 'resolve_existing_files': True, 'updateStrategy': 'replace'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2022 02:12:05 PM - INFO: Ingest donor response TDRResponse(id='rbwpgO2iTF6tGZXOsxannQ', job_status='succeeded', status_code=200, description='Ingest from gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/donor.json to donor in dataset id fce017a3-1fc0-4cd6-9638-6a1f8dcd1c09', submitted='2022-05-11T14:11:35.008521Z', completed='2022-05-11T14:11:56.177675Z', class_name='bio.terra.service.dataset.flight.ingest.DatasetIngestFlight')\n",
      "05/11/2022 02:12:05 PM - INFO: Checking file ingest_pipeline/output/tim_core/metadata/familymember.json\n",
      "05/11/2022 02:12:05 PM - INFO: Running ingest: familymember\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'json': {'table': 'familymember', 'format': 'json', 'path': 'gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/familymember.json', 'resolve_existing_files': True, 'updateStrategy': 'replace'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2022 02:12:35 PM - INFO: Ingest familymember response TDRResponse(id='lB07Fc_oREKeW5nrLfMBGA', job_status='succeeded', status_code=200, description='Ingest from gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/familymember.json to familymember in dataset id fce017a3-1fc0-4cd6-9638-6a1f8dcd1c09', submitted='2022-05-11T14:12:05.462133Z', completed='2022-05-11T14:12:24.970298Z', class_name='bio.terra.service.dataset.flight.ingest.DatasetIngestFlight')\n",
      "05/11/2022 02:12:35 PM - INFO: Checking file ingest_pipeline/output/tim_core/metadata/diagnosis.json\n",
      "05/11/2022 02:12:36 PM - INFO: Running ingest: diagnosis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'json': {'table': 'diagnosis', 'format': 'json', 'path': 'gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/diagnosis.json', 'resolve_existing_files': True, 'updateStrategy': 'replace'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2022 02:13:06 PM - INFO: Ingest diagnosis response TDRResponse(id='8r2KWmo7TwGtzy4kXDZ9ew', job_status='succeeded', status_code=200, description='Ingest from gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/diagnosis.json to diagnosis in dataset id fce017a3-1fc0-4cd6-9638-6a1f8dcd1c09', submitted='2022-05-11T14:12:36.262334Z', completed='2022-05-11T14:12:56.687016Z', class_name='bio.terra.service.dataset.flight.ingest.DatasetIngestFlight')\n",
      "05/11/2022 02:13:06 PM - INFO: Checking file ingest_pipeline/output/tim_core/metadata/biosample.json\n",
      "05/11/2022 02:13:06 PM - INFO: Running ingest: biosample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'json': {'table': 'biosample', 'format': 'json', 'path': 'gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/biosample.json', 'resolve_existing_files': True, 'updateStrategy': 'replace'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2022 02:13:37 PM - INFO: Ingest biosample response TDRResponse(id='F8IU1aqlRvWcSgFwbhE6Uw', job_status='succeeded', status_code=200, description='Ingest from gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/biosample.json to biosample in dataset id fce017a3-1fc0-4cd6-9638-6a1f8dcd1c09', submitted='2022-05-11T14:13:06.774206Z', completed='2022-05-11T14:13:27.602732Z', class_name='bio.terra.service.dataset.flight.ingest.DatasetIngestFlight')\n",
      "05/11/2022 02:13:37 PM - INFO: Checking file ingest_pipeline/output/tim_core/metadata/sequencingactivity.json\n",
      "05/11/2022 02:13:37 PM - INFO: Running ingest: sequencingactivity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'json': {'table': 'sequencingactivity', 'format': 'json', 'path': 'gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/sequencingactivity.json', 'resolve_existing_files': True, 'updateStrategy': 'replace'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2022 02:14:08 PM - INFO: Ingest sequencingactivity response TDRResponse(id='_opvVDIzQWeNZnhF7pgwKA', job_status='succeeded', status_code=200, description='Ingest from gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/sequencingactivity.json to sequencingactivity in dataset id fce017a3-1fc0-4cd6-9638-6a1f8dcd1c09', submitted='2022-05-11T14:13:37.934812Z', completed='2022-05-11T14:13:57.292291Z', class_name='bio.terra.service.dataset.flight.ingest.DatasetIngestFlight')\n",
      "05/11/2022 02:14:08 PM - INFO: Checking file ingest_pipeline/output/tim_core/metadata/file.json\n",
      "05/11/2022 02:14:08 PM - INFO: Running ingest: file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'json': {'table': 'file', 'format': 'json', 'path': 'gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/file.json', 'resolve_existing_files': True, 'updateStrategy': 'replace'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2022 02:15:09 PM - INFO: Ingest file response TDRResponse(id='PaQLJQIkRWuP3v3CoP7D0Q', job_status='succeeded', status_code=200, description='Ingest from gs://fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46/ingest_pipeline/output/tim_core/metadata/file.json to file in dataset id fce017a3-1fc0-4cd6-9638-6a1f8dcd1c09', submitted='2022-05-11T14:14:08.427997Z', completed='2022-05-11T14:14:51.376334Z', class_name='bio.terra.service.dataset.flight.ingest.DatasetIngestFlight')\n",
      "05/11/2022 02:15:09 PM - INFO: Checking file ingest_pipeline/output/tim_core/metadata/variantcall.json\n",
      "05/11/2022 02:15:09 PM - WARNING: Metadata file does not exist.  Skipping: variantcall\n",
      "05/11/2022 02:15:09 PM - INFO: Checking file ingest_pipeline/output/tim_core/metadata/dataset.json\n",
      "05/11/2022 02:15:09 PM - WARNING: Metadata file does not exist.  Skipping: dataset\n",
      "05/11/2022 02:15:09 PM - INFO: Checking file ingest_pipeline/output/tim_core/metadata/project.json\n",
      "05/11/2022 02:15:09 PM - WARNING: Metadata file does not exist.  Skipping: project\n",
      "05/11/2022 02:15:10 PM - INFO: Adding {\"email\": \"prod-dsp-data-ingest@firecloud.org\"} as Steward to dataset.\n",
      "05/11/2022 02:15:10 PM - INFO: Running output data profiling\n",
      "05/11/2022 02:15:10 PM - INFO: Building and executing table-level queries.\n",
      "05/11/2022 02:15:29 PM - INFO: Building and executing column-level queries.\n",
      "05/11/2022 02:31:20 PM - INFO: Building and executing orphan file queries.\n",
      "05/11/2022 02:31:23 PM - INFO: Writing out results to ingest_pipeline/output/tim_core/validation/datarepo_anvil_cmg_ingest_resources_TEST_metric_results.csv.\n",
      "05/11/2022 02:31:26 PM - INFO: Yay! The pipeline has completed!\n"
     ]
    }
   ],
   "source": [
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Donor Transformations using transform_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donor Transformations\n",
    "src_file1 = TransformerSource(file_name=\"subject.tsv\", primary_key=\"subject_id\", rename_primary_key=\"entity:subject_id\")\n",
    "d_id = TransformerMap(source_column=\"subject_id\", target_column=\"donor_id\")\n",
    "e_id = TransformerMap(source_column=\"ancestry\", target_column=\"reported_ethnicity\")\n",
    "p_id = TransformerMap(source_column=\"sex\", target_column=\"phenotypic_sex\")\n",
    "s_id = TransformerMap(source_column=\"sex\", target_column=\"sex_assigned_at_birth\")\n",
    "sf_id = TransformerMap(source_column=\"twin_id\", target_column=\"sibling_familymember_id\")\n",
    "\n",
    "maps = [d_id, e_id, p_id, s_id, sf_id]\n",
    "\n",
    "diag_tf = TransformerTransform(source_columns=[\"donor_id\", \"disease_id\"], target_column=\"diagnosis_id\", transform_type=TransformType.CONCAT_STR_TO_LIST_PREFIX)\n",
    "fam_tf = TransformerTransform(source_columns=[\"family_id\"], target_column=\"family_id\", transform_type=TransformType.COLS_TO_LIST)\n",
    "bpf_tf = TransformerTransform(source_columns=[\"maternal_id\", \"paternal_id\"], target_column=\"parent_familymember_id\", transform_type=TransformType.COLS_TO_LIST)\n",
    "\n",
    "tfs = [diag_tf, bpf_tf, fam_tf]\n",
    "\n",
    "donor_pass = ['project_id', 'prior_testing', 'age_at_last_observation', 'congenital_status', 'multiple_datasets', 'ancestry_detail', 'submitter_id', 'dbgap_submission', 'affected_status']\n",
    "\n",
    "tf_input_dir = \"ingest_pipeline/input/metadata\"\n",
    "tf_output_dir = \"ingest_pipeline/output/tim_core/metadata\"\n",
    "\n",
    "input_dir = f\"{ws_bucket}/{tf_input_dir}\"\n",
    "req_donor = TransformerRequest(input_directory=input_dir, output_directory=tf_output_dir, source_files=[src_file1], destination_table=\"donor\", passthrough_cols=donor_pass, maps=maps, transforms=tfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Run transform_task transform using above request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = transform(req_donor)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Donor Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_path = f\"{ws_bucket}/{tf_output_dir}/donor.json\"\n",
    "\n",
    "src_file_path2 = f\"{ws_bucket}/{tf_output_dir}/donor2.json\"\n",
    "df = pd.read_json(src_file_path, lines=True)\n",
    "df2 = pd.read_json(src_file_path2, lines=True)\n",
    "\n",
    "df2 = df2.drop(columns=[\"family_id\"])\n",
    "\n",
    "df = df.sort_index(axis=1)\n",
    "df2 = df2.sort_index(axis=1)\n",
    "\n",
    "#print(\"df\")\n",
    "#for col in df.columns:\n",
    "    #print(col)\n",
    "    \n",
    "#print(\"df2\")\n",
    "#for col in df2.columns:\n",
    "    #print(col)\n",
    "\n",
    "df.equals(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
