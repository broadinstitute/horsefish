{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1:  set up \"global\" imports and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace name = tdr-anvil-ingest-bjt\n",
      "workspace project = dsp-data-ingest\n",
      "workspace bucket = gs://fc-secure-e7856519-5bea-4fec-88ec-dad61673d22f\n"
     ]
    }
   ],
   "source": [
    "## imports and environment variables\n",
    "# imports\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "\n",
    "print(f\"workspace name = {ws_name}\")\n",
    "print(f\"workspace project = {ws_project}\")\n",
    "print(f\"workspace bucket = {ws_bucket}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2: get source data files and complete basic validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_source_files():\n",
    "    # gets list of entity types in workspace\n",
    "\n",
    "    # API call to get all entity types in workspace\n",
    "    response_etypes = fapi.list_entity_types(ws_project, ws_name)\n",
    "    dict_all_etypes = json.loads(response_etypes.text)\n",
    "\n",
    "    etypes_list = []\n",
    "    etypes_list = [key for key in dict_all_etypes.keys()]\n",
    "\n",
    "    print(f\"List of entity types in current workspace:\")\n",
    "    print('\\n'.join(['\\t' * 5 + c for c in etypes_list]))\n",
    "    \n",
    "    # for each entity type, download tsv file to notebook PD\n",
    "\n",
    "    # initiate validation data list to capture entity counts in workspace vs counts in tsv\n",
    "    # want to compare that all rows successfully downloaded to tsvs\n",
    "    # items in list = [{\"entity_type\": \"table_name\", \"data_model_count\": #, \"tsv_file_count\": #},{...}]\n",
    "    validation_data = []\n",
    "\n",
    "    for etype in etypes_list:\n",
    "        print(f'Starting download of tsv file for {etype}.')\n",
    "\n",
    "        # get entity table response for API call\n",
    "        res_etype = fapi.get_entities_tsv(ws_project, ws_name, etype, model=\"flexible\")\n",
    "\n",
    "        # Save current/original data model tsv files to the bucket for provenance\n",
    "        destination_dir = \"ingest_pipeline/input/metadata\"\n",
    "        print(f'Saving original {etype} TSV to {ws_bucket}/{destination_dir}')\n",
    "        original_tsv_name = etype + \".tsv\"\n",
    "        with open(original_tsv_name, \"w\") as f:\n",
    "            f.write(res_etype.text)\n",
    "\n",
    "        # get number of rows in downloaded tsv file for given entity and update validation dict with count\n",
    "        num_tsv_entities = !tail -n +2 $original_tsv_name | wc -l\n",
    "\n",
    "        # capture counts of given entity into dictionary\n",
    "        validation_dict = {}\n",
    "        validation_dict[\"entity_type\"] = etype\n",
    "        validation_dict[\"tsv_file_count\"] = num_tsv_entities[0]\n",
    "        validation_dict[\"data_model_count\"] = dict_all_etypes[etype][\"count\"]\n",
    "\n",
    "        validation_data.append(validation_dict)\n",
    "\n",
    "        # copy files to workspace bucket\n",
    "        !gsutil cp $original_tsv_name $ws_bucket/$destination_dir/ 2> stdout\n",
    "        \n",
    "        ## print validation dataframe for visual inspection\n",
    "\n",
    "    # set values differ because we have to determine how to get set files downloaded\n",
    "    # TODO: only list rows where the numbers don't match or highlight rows where numbers don't match up\n",
    "    validation_df = pd.DataFrame(validation_data)\n",
    "    print(f\"source files validation metrics: \\n\")\n",
    "    print(validation_df)\n",
    "\n",
    "    # write validation df to file and copy file to destination_dir\n",
    "    validation_metrics_filename = f\"{ws_name}_src_file_validation_metrics.csv\"\n",
    "    validation_df.to_csv(validation_metrics_filename, sep='\\t')\n",
    "\n",
    "    !gsutil cp $validation_metrics_filename $ws_bucket/$destination_dir/\n",
    "\n",
    "    print(f\"\\n validation metrics: {ws_bucket}/{destination_dir}/{validation_metrics_filename}\")\n",
    "    \n",
    "    # delete copy of tsv files from notebook env - they will persist in designated workspace bucket directory\n",
    "    !rm *.tsv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3: get workspace metadata and dataset attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get workspace attributes and phs ID functions\n",
    "def get_workspace_attributes(ws_project, ws_name):\n",
    "    \"\"\"Get workspace attributes, write to json, copy json to workspace bucket.\"\"\"\n",
    "    \n",
    "    ws_attributes = fapi.get_workspace(ws_project, ws_name, fields=\"workspace.attributes \\n\").json()\n",
    "    \n",
    "    destination_dir = \"ingest_pipeline/input/metadata\"\n",
    "\n",
    "    # write json to file and save in directory with metadata\n",
    "    ws_attrs_filename = f\"{ws_name}_workspace_attributes.json\"\n",
    "    with open(ws_attrs_filename, 'w') as json_outfile:\n",
    "        json.dump(ws_attributes, json_outfile)\n",
    "        # copy json file to bucket\n",
    "        !gsutil cp $ws_attrs_filename $ws_bucket/$destination_dir/ 2> stdout\n",
    "        print(f\"workspace attributes: {ws_bucket}/{destination_dir}/{ws_attrs_filename}\")\n",
    "    \n",
    "    return ws_attributes\n",
    "\n",
    "def get_workspace_phs_id(ws_attributes):\n",
    "    \"\"\"Get workspace's phs ID from workspace attributes.\"\"\"\n",
    "\n",
    "    # parse workspace attributes to get phsID from tags\n",
    "    tags_list = ws_attributes[\"workspace\"][\"attributes\"][\"tag:tags\"][\"items\"]\n",
    "\n",
    "    tags_dict = {}\n",
    "    for key_value in tags_list:\n",
    "        key, value = key_value.split(': ', 1)\n",
    "        tags_dict[key] = value\n",
    "\n",
    "    phs_id = tags_dict[\"dbGaP\"]\n",
    "    print(f\"phs ID for workspace: {phs_id}\")\n",
    "    \n",
    "    return phs_id\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
