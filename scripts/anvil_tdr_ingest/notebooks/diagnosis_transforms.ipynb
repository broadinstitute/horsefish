{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports and environment variables\n",
    "# imports\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "import logging\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform functions\n",
    "\n",
    "# Function to convert list represented as string to a list data type\n",
    "def str_list_to_list(in_str, list_delim):\n",
    "    out_list = []\n",
    "    out_list = in_str.split(sep=list_delim)\n",
    "    return out_list\n",
    "\n",
    "# Function to concatenate a string value to each entry in a list (either 'prefix' or 'suffix')\n",
    "def concat_str_to_list(in_str, in_list, delim='_', mode='prefix'):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if mode == 'prefix':\n",
    "            out_list.append(in_str + delim + item)\n",
    "        elif mode == 'suffix':\n",
    "            out_list.append(item + delim + instr)\n",
    "        else:\n",
    "            out_list.append(item)\n",
    "    return out_list\n",
    "\n",
    "# Function to convert non-null values from a list of columns into a list\n",
    "def df_cols_to_list(in_list):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if pd.notnull(item):\n",
    "            out_list.append(item)\n",
    "    return out_list\n",
    "\n",
    "# Function to return matching value from list\n",
    "def get_match_val_in_list(search_val, search_list):\n",
    "    if len(search_list) == 0:\n",
    "        return None\n",
    "    elif len(search_list) == 1:\n",
    "        return search_list[0]\n",
    "    else:\n",
    "        return_str = ''\n",
    "        for item in search_list:\n",
    "            if item in search_val:\n",
    "                return_str = item\n",
    "                break\n",
    "        return return_str\n",
    "\n",
    "#print(get_match_val_in_list('sub-94546_OMIM:313850', ['OMIM:313850', 'OMIM:123456']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(params):\n",
    "    \n",
    "    # Retrieve parameters of interest\n",
    "    tf_input_dir = params[\"tf_input_dir\"]\n",
    "    tf_output_dir = params[\"tf_output_dir\"]\n",
    "    \n",
    "    # Attempt to read source files into data frame, checking for missing files or key fields \n",
    "    try:\n",
    "        src_file = 'subject.tsv'\n",
    "        src_file_path = ws_bucket + '/' + tf_input_dir + '/' + src_file\n",
    "        df = pd.read_csv(src_file_path, delimiter = '\\t').rename(columns = {'entity:subject_id':'subject_id'})\n",
    "        key_fields = ['subject_id', 'disease_id']\n",
    "        missing_key_fields = []\n",
    "        for item in key_fields:\n",
    "            if item not in df.columns:\n",
    "                missing_key_fields.append(item)\n",
    "        if len(missing_key_fields) > 0:\n",
    "            missing_fields_str = ', '.join(missing_key_fields)\n",
    "            logging.error('Key source fields ({fields}) not found in file ({file}).'.format(fields = missing_fields_str, file = src_file))\n",
    "            return\n",
    "    except:\n",
    "        logging.error('Source file {src} not found.'.format(src = src_file))\n",
    "        return\n",
    "    \n",
    "    # Transform mapped fields (appending new fields to end of existing DF for now)\n",
    "    if {'subject_id', 'disease_id'}.issubset(df.columns):\n",
    "        df['diagnosis_id'] = df.apply(lambda x: concat_str_to_list(str(x['subject_id']), str_list_to_list(str(x['disease_id']), '|'), '_', 'prefix') if(pd.notnull(x['disease_id'])) else x['disease_id'], axis=1)\n",
    "    if {'age_of_onset'}.issubset(df.columns):\n",
    "        df['onset_age_upper_bound'] = df['age_of_onset']\n",
    "        df['onset_age_lower_bound'] = df['age_of_onset']\n",
    "    if {'disease_id'}.issubset(df.columns):\n",
    "        df['disease_other'] = df.apply(lambda x: str_list_to_list(str(x['disease_id']), '|') if(pd.notnull(x['disease_id'])) else [], axis=1)\n",
    "    \n",
    "    # Explode dataframe on diagnosis id so there is one record per diagnosis_id\n",
    "    explode_cols = ['diagnosis_id', 'disease_other']\n",
    "    explode_cols_final = []\n",
    "    for item in explode_cols:\n",
    "        if item in df.columns:\n",
    "            explode_cols_final.append(item)\n",
    "    df2 = df.loc[pd.notna(df['diagnosis_id'])].explode(explode_cols, ignore_index=True)\n",
    "\n",
    "    # Set disease_other to be an array field\n",
    "    df2['disease_other'] = df2.apply(lambda x: [x['disease_other']] if(len(str(x['disease_other'])) > 0) else [], axis=1)\n",
    "    \n",
    "    # Limit DF to transformed and passthrough fields\n",
    "    mapped_columns = ['diagnosis_id', 'onset_age_lower_bound', 'onset_age_upper_bound', 'disease_other']\n",
    "    passthrough_columns = ['disease_description', 'solve_state', 'hpo_present', 'hpo_absent']\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df2.columns:\n",
    "            final_col_list.append(item)\n",
    "    for item in passthrough_columns:\n",
    "        if item in df2.columns:\n",
    "            final_col_list.append(item)\n",
    "    df3 = df2[final_col_list] # Creating to avoid any cardinality issues when rejoining the passthrough data in the subsequent steps\n",
    "    \n",
    "    # Build passthrough string \n",
    "    passthrough_col_list = []\n",
    "    for item in passthrough_columns:\n",
    "        if item in df2.columns:\n",
    "            passthrough_col_list.append(item)\n",
    "    passthrough_col_list.sort()\n",
    "    passthrough_df = df2[passthrough_col_list]\n",
    "    add_data_df = passthrough_df.apply(lambda x: x.to_json(), axis=1).to_frame()\n",
    "    add_data_df.columns = ['additional_data']\n",
    "\n",
    "    # Merge mapped columns with additional data column to build final df\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df.columns:\n",
    "            final_col_list.append(item)\n",
    "    df_final = df2[final_col_list].join(add_data_df)\n",
    "\n",
    "    # Convert dataframe to new-line delimited JSON and write out to file\n",
    "    destination_dir = tf_output_dir\n",
    "    destination_file = 'diagnosis.json'\n",
    "    records_json = df_final.to_json(orient='records') # Converting to JSON string first to replace NaN with nulls\n",
    "    records_list = json.loads(records_json)\n",
    "    records_cnt = len(records_list)\n",
    "\n",
    "    #print(records_cnt)\n",
    "    #print(records_json)\n",
    "    #print(records_list)\n",
    "\n",
    "    with open(destination_file, 'w') as outfile:\n",
    "        for idx, val in enumerate(records_list):\n",
    "            json.dump(val, outfile) # Adds escape characters to additional_data field --> Not sure it's a problem\n",
    "            if idx < (records_cnt - 1):\n",
    "                outfile.write('\\n')\n",
    "\n",
    "    # Copy file to workspace bucket\n",
    "    !gsutil cp $destination_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "    \n",
    "    # Delete tsv files from notebook env - they will persist in designated workspace bucket directory\n",
    "    !rm $destination_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# params[\"tf_input_dir\"] = \"ingest_pipeline/input/metadata\"\n",
    "# params[\"tf_output_dir\"] = \"ingest_pipeline/output/tim_core/metadata\"\n",
    "# transform(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
