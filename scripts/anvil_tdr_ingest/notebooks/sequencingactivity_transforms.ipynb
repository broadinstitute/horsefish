{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports and environment variables\n",
    "# imports\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "import re\n",
    "import hashlib\n",
    "import logging\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform functions\n",
    "\n",
    "# Function to convert list represented as string to a list data type\n",
    "def str_list_to_list(in_str, list_delim):\n",
    "    out_list = []\n",
    "    out_list = in_str.split(sep=list_delim)\n",
    "    return out_list\n",
    "\n",
    "# Function to concatenate a string value to each entry in a list (either 'prefix' or 'suffix')\n",
    "def concat_str_to_list(in_str, in_list, delim='_', mode='prefix'):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if mode == 'prefix':\n",
    "            out_list.append(in_str + delim + item)\n",
    "        elif mode == 'suffix':\n",
    "            out_list.append(item + delim + instr)\n",
    "        else:\n",
    "            out_list.append(item)\n",
    "    return out_list\n",
    "\n",
    "# Function to convert non-null values from a list of columns into a list\n",
    "def df_cols_to_list(in_list):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if pd.notnull(item):\n",
    "            out_list.append(item)\n",
    "    return out_list\n",
    "\n",
    "# Function to add value to existing list (or create new list)\n",
    "def add_to_list(curr_value, new_value):\n",
    "    return_list = []\n",
    "    if new_value == None:\n",
    "        if type(curr_value) == list:\n",
    "            return_list = curr_value\n",
    "        else:\n",
    "            return_list.append(curr_value)\n",
    "    elif type(new_value) == list:\n",
    "        if curr_value == None:\n",
    "            return_list = new_value\n",
    "        elif type(curr_value) == list:\n",
    "            return_list = curr_value\n",
    "            for item in new_value:\n",
    "                if item not in curr_value:\n",
    "                    return_list.append(item)      \n",
    "        elif type(curr_value) != list:\n",
    "            return_list.append(curr_value)\n",
    "            for item in new_value:\n",
    "                if item != curr_value:\n",
    "                    return_list.append(item) \n",
    "    elif type(new_value) != list:\n",
    "        if curr_value == None:\n",
    "            return_list.append(new_value)\n",
    "        elif type(curr_value) == list:\n",
    "            return_list = curr_value\n",
    "            if new_value not in curr_value:\n",
    "                return_list.append(new_value)         \n",
    "        elif type(curr_value) != list:\n",
    "            return_list.append(curr_value)\n",
    "            if new_value != curr_value:\n",
    "                return_list.append(new_value)\n",
    "    return return_list\n",
    "\n",
    "# Function to build file reference for file based on a given file name or path\n",
    "def find_file_in_manifest(search_string, file_manifest):\n",
    "    # Loop through file manifest and record fileref_obj_entry where matches are found\n",
    "    fileref_obj = []\n",
    "    match_cnt = 0\n",
    "    for entry in file_manifest:\n",
    "        file_path = ''\n",
    "        if search_string in entry['name']:\n",
    "            match_cnt += 1\n",
    "            fileref_obj.append(entry['file_id'])\n",
    "    \n",
    "    # Return fileref object\n",
    "    if match_cnt == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return fileref_obj\n",
    "\n",
    "#print(find_file_in_manifest('112610'))\n",
    "#print(add_to_list(None, find_file_in_manifest('112610')))\n",
    "\n",
    "# Function to return objects in specified bucket\n",
    "def get_objects_list(bucket_name, dirs_to_exclude=[], dirs_to_include=[]):\n",
    "    \n",
    "    # Collect list of objects/blobs from bucket \n",
    "    obj_list = []\n",
    "    storage_client = storage.Client()\n",
    "    storage_bucket = storage_client.bucket(bucket_name, user_project='dsp-data-ingest')\n",
    "    objects = list(storage_client.list_blobs(storage_bucket))\n",
    "    \n",
    "    # Loop through list of objects and append names to final list based on the roots_to_include and roots_to_exclude variables\n",
    "    for obj in objects:\n",
    "        obj_root = obj.name.split('/')[0]\n",
    "        if len(dirs_to_include) > 0:\n",
    "            for entry in dirs_to_include:\n",
    "                if entry in obj.name:\n",
    "                    obj_list.append(obj.name)\n",
    "        elif len(dirs_to_exclude) > 0:\n",
    "            for entry in dirs_to_exclude:\n",
    "                if entry not in obj.name:\n",
    "                    obj_list.append(obj.name)\n",
    "        else:\n",
    "            obj_list.append(obj.name)\n",
    "    return obj_list\n",
    "\n",
    "# Function to return object metadata\n",
    "def get_object(bucket_name, object_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name, user_project='dsp-data-ingest')\n",
    "    obj = bucket.get_blob(object_name)\n",
    "    return obj\n",
    "\n",
    "# Function to pull full file extension (including compression extensions)\n",
    "def get_full_file_ext(filepath):\n",
    "    full_ext_string = filepath\n",
    "    compression_extension = ''\n",
    "    compression_extensions = ['.7z', '.zip', '.gz', '.tar.gz', '.tgz']\n",
    "    for item in compression_extensions:\n",
    "        pattern = item + '$'\n",
    "        if re.search(pattern, full_ext_string):\n",
    "            full_ext_string = re.sub(pattern, '', full_ext_string)\n",
    "            compression_extension = item\n",
    "            break\n",
    "    full_ext_string = os.path.splitext(full_ext_string)[1] + compression_extension\n",
    "    return full_ext_string\n",
    "\n",
    "# Function to build file manifest\n",
    "def build_manifest(params):\n",
    "\n",
    "    # Collect parameters\n",
    "    data_files_src_bucket = params[\"data_files_src_bucket\"]\n",
    "    data_files_src_dirs = params[\"data_files_src_dirs\"]\n",
    "    data_files_src_dirs_exclude = params[\"data_files_src_dirs_exclude\"]\n",
    "    \n",
    "    # Define record list\n",
    "    record_list = []\n",
    "\n",
    "    # Loop through object list to construct manifest entry for each non-directory object \n",
    "    if data_files_src_bucket == None:\n",
    "        data_files_src_bucket = ws_bucket_name\n",
    "    object_list = get_objects_list(data_files_src_bucket, data_files_src_dirs_exclude, data_files_src_dirs)\n",
    "    for entry in object_list:\n",
    "        if not re.search('/$', entry):\n",
    "            # Collect information for manifest entry record\n",
    "            entry_obj_record = []\n",
    "            entry_obj = get_object(data_files_src_bucket, entry)\n",
    "            entry_obj_uri = 'gs://' + data_files_src_bucket + '/' + entry_obj.name\n",
    "            entry_obj_id_str = entry_obj_uri + entry_obj.md5_hash\n",
    "            entry_obj_id = hashlib.md5(entry_obj_id_str.encode())\n",
    "            entry_obj_file_name = os.path.split(entry_obj.name)[1]\n",
    "            entry_obj_full_ext = get_full_file_ext(entry_obj_file_name)\n",
    "            # Construct fileref object\n",
    "            fileref_obj = {}\n",
    "            fileref_obj['sourcePath'] = entry_obj_uri\n",
    "            fileref_obj['targetPath'] = ('/' + entry_obj.name).replace('//', '/')\n",
    "            fileref_obj['description'] = f'Ingest of {entry_obj_uri}'\n",
    "            fileref_obj['mimeType'] = 'text/plain'\n",
    "            # Construct manifest entry record and append to record list\n",
    "            entry_obj_record = [entry_obj_id.hexdigest(), entry_obj_file_name, entry_obj.name, entry_obj_uri, entry_obj.content_type, entry_obj_full_ext, entry_obj.size, entry_obj.crc32c, entry_obj.md5_hash, fileref_obj]  \n",
    "            record_list.append(entry_obj_record)\n",
    "\n",
    "    # Build manifest dataframe, drop duplicates, and build JSON object\n",
    "    column_list = ['file_id', 'name', 'path', 'uri', 'content_type', 'full_extension', 'size_in_bytes', 'crc32c', 'md5_hash', 'file_ref']\n",
    "    df_file_manifest = pd.DataFrame(record_list, columns = column_list)\n",
    "    df_file_manifest.drop_duplicates(['name', 'md5_hash'], keep='first', inplace=True, ignore_index=True)\n",
    "    file_manifest = df_file_manifest.to_dict(orient='records')\n",
    "    return file_manifest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(params):\n",
    "    \n",
    "    # Retrieve parameters of interest\n",
    "    tf_input_dir = params[\"tf_input_dir\"]\n",
    "    tf_output_dir = params[\"tf_output_dir\"]\n",
    "    data_files_src_bucket = params[\"data_files_src_bucket\"]\n",
    "    data_files_src_dirs = params[\"data_files_src_dirs\"]\n",
    "    data_files_src_dirs_exclude = params[\"data_files_src_dirs_exclude\"]\n",
    "    fileref_columns = params[\"fileref_columns\"]\n",
    "    file_manifest = params[\"file_manifest\"]\n",
    "\n",
    "    # Attempt to read source files into data frame, checking for missing files or key fields \n",
    "    try:\n",
    "        src_file = 'sequencing.tsv'\n",
    "        src_file_path = ws_bucket + '/' + tf_input_dir + '/' + src_file\n",
    "        df_seq = pd.read_csv(src_file_path, delimiter = '\\t').rename(columns = {'entity:sequencing_id':'sequencing_id'})\n",
    "        key_fields = ['sequencing_id']\n",
    "        missing_key_fields = []\n",
    "        for item in key_fields:\n",
    "            if item not in df_seq.columns:\n",
    "                missing_key_fields.append(item)\n",
    "        if len(missing_key_fields) > 0:\n",
    "            missing_fields_str = ', '.join(missing_key_fields)\n",
    "            logging.error('Key source fields ({fields}) not found in file ({file}).'.format(fields = missing_fields_str, file = src_file))\n",
    "            return\n",
    "    except:\n",
    "        logging.error('Source file {src} not found.'.format(src = src_file))\n",
    "        return\n",
    "    \n",
    "    # Transform mapped fields (appending new fields to end of existing DF for now)\n",
    "    if {'sequencing_id'}.issubset(df_seq.columns):\n",
    "        df_seq['sequencingactivity_id'] = df_seq['sequencing_id']\n",
    "    if {'library_prep_kit_method'}.issubset(df_seq.columns):\n",
    "        df_seq['protocol'] = df_seq.apply(lambda x: [x['library_prep_kit_method']] if(pd.notnull(x['library_prep_kit_method'])) else [], axis=1)  \n",
    "    if {'reference_genome_build'}.issubset(df_seq.columns):\n",
    "        df_seq['reference_assembly'] = df_seq.apply(lambda x: [x['reference_genome_build']] if(pd.notnull(x['reference_genome_build'])) else [], axis=1)  \n",
    "    if {'sample_id'}.issubset(df_seq.columns):\n",
    "        df_seq['uses_sample_biosample_id'] = df_seq.apply(lambda x: [x['sample_id']] if(pd.notnull(x['sample_id'])) else [], axis=1) \n",
    "    if {'sequencing_platform'}.issubset(df_seq.columns):\n",
    "        df_seq['platform'] = df_seq['sequencing_platform']\n",
    "    if {'alignment_method', 'data_processing_pipeline'}.issubset(df_seq.columns):\n",
    "        df_seq['used'] = df_seq.apply(lambda x: df_cols_to_list([x['alignment_method'], x['data_processing_pipeline']]), axis=1)\n",
    "    elif {'alignment_method'}.issubset(df_seq.columns):\n",
    "        df_seq['used'] = df_seq.apply(lambda x: [x['alignment_method']] if(pd.notnull(x['alignment_method'])) else [], axis=1)\n",
    "    elif {'data_processing_pipeline'}.issubset(df_seq.columns):\n",
    "        df_seq['used'] = df_seq.apply(lambda x: [x['data_processing_pipeline']] if(pd.notnull(x['data_processing_pipeline'])) else [], axis=1) \n",
    "    if {'sequencing_strategy'}.issubset(df_seq.columns):\n",
    "        df_seq['library_layout'] = df_seq['sequencing_strategy']\n",
    "    \n",
    "    # File ref columns\n",
    "    if any(item in df_seq.columns for item in fileref_columns):\n",
    "        df_seq['generated_file_id'] = [[] for _ in range(len(df_seq))]\n",
    "        for item in fileref_columns:\n",
    "            if {item}.issubset(df_seq.columns):\n",
    "                df_seq['generated_file_id'] = df_seq.apply(lambda x: add_to_list(x['generated_file_id'], find_file_in_manifest(x[item], file_manifest)) if(pd.notnull(x[item])) else x['generated_file_id'], axis=1)\n",
    "    \n",
    "    # Limit DF to transformed and passthrough fields\n",
    "    mapped_columns = ['sequencingactivity_id', 'protocol', 'reference_assembly', 'uses_sample_biosample_id', 'generated_file_id', 'platform', 'used', 'library_layout']\n",
    "    passthrough_columns = ['analyte_type', 'exome_capture_platform', 'target_insert_size', 'target_depth', 'read_length', 'ancestry_detail', 'number_of_independent_libraries', 'sex', 'submitter_id', 'sequencing_center', 'capture_region_bed_file', 'date_data_generation', 'functional_equivalence_standard', 'seq_filename', 'sequencer_id', 'sequencing_assay', 'tissue_source']\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df_seq.columns:\n",
    "            final_col_list.append(item)\n",
    "    for item in passthrough_columns:\n",
    "        if item in df_seq.columns:\n",
    "            final_col_list.append(item)\n",
    "    df_seq2 = df_seq[final_col_list] # Creating to avoid any cardinality issues when rejoining the passthrough data in the subsequent steps\n",
    "\n",
    "    # Build passthrough string \n",
    "    passthrough_col_list = []\n",
    "    for item in passthrough_columns:\n",
    "        if item in df_seq2.columns:\n",
    "            passthrough_col_list.append(item)\n",
    "    passthrough_col_list.sort()\n",
    "    passthrough_df_seq = df_seq2[passthrough_col_list]\n",
    "    add_data_df_seq = passthrough_df_seq.apply(lambda x: x.to_json(), axis=1).to_frame()\n",
    "    add_data_df_seq.columns = ['additional_data']\n",
    "    \n",
    "    # Merge mapped columns with additional data column to build final df\n",
    "    final_col_list = []\n",
    "    for item in mapped_columns:\n",
    "        if item in df_seq.columns:\n",
    "            final_col_list.append(item)\n",
    "    df_seq_final = df_seq[final_col_list].join(add_data_df_seq)\n",
    "    \n",
    "    # Convert dataframe to new-line delimited JSON and write out to file\n",
    "    destination_dir = tf_output_dir\n",
    "    destination_file = 'sequencingactivity.json'\n",
    "    records_json = df_seq_final.to_json(orient='records') # Converting to JSON string first to replace NaN with nulls\n",
    "    records_list = json.loads(records_json)\n",
    "    records_cnt = len(records_list)\n",
    "\n",
    "    with open(destination_file, 'w') as outfile:\n",
    "        for idx, val in enumerate(records_list):\n",
    "            json.dump(val, outfile) # Adds escape characters to additional_data field --> Not sure it's a problem\n",
    "            if idx < (records_cnt - 1):\n",
    "                outfile.write('\\n')\n",
    "    \n",
    "    # Copy file to workspace bucket\n",
    "    !gsutil cp $destination_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "    \n",
    "    # Delete tsv files from notebook env - they will persist in designated workspace bucket directory\n",
    "    !rm $destination_file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# params[\"tf_input_dir\"] = \"ingest_pipeline/input/metadata\"\n",
    "# params[\"tf_output_dir\"] = \"ingest_pipeline/output/tim_core/metadata\"\n",
    "# params[\"data_files_src_bucket\"] = \"fc-9cd4583e-7855-4b5e-ae88-d8971cfd5b46\"\n",
    "# params[\"data_files_src_dirs\"] = []  # Leave empty to include all\n",
    "# params[\"data_files_src_dirs_exclude\"] = [] \n",
    "# params[\"fileref_columns\"] = [\"sequencing_id\", \"seq_filename\", \"capture_region_bed_file\"]\n",
    "# params[\"file_manifest\"] = []\n",
    "# #params[\"file_manifest\"] = build_manifest(params)\n",
    "# transform(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
